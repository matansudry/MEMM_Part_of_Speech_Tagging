{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import inspect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from utils import preprocess, features, classifier, metrics, logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "models_path = 'models'  # folder of models\n",
    "log_path = 'log'  # folder of log.csv\n",
    "logger = logging.Logger(log_path)  # init logger to log.csv\n",
    "# logger.init_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets():\n",
    "    test1_path = 'data/test1.wtag'\n",
    "    train1_path = 'data/train1.wtag'\n",
    "    train2_path = 'data/train2.wtag'\n",
    "    comp1_path = 'data/comp1_tagged.words'  # need to tag dataset to use\n",
    "    comp2_path = 'data/comp2_tagged.words'  # need to tag dataset to use\n",
    "\n",
    "    train_dataset = preprocess.Dataset(train1_path)\n",
    "#     train_dataset = preprocess.Dataset(train2_path)\n",
    "    val_dataset = preprocess.Dataset(test1_path)\n",
    "#     val_dataset = preprocess.Dataset(comp1_path)\n",
    "#     val_dataset = preprocess.Dataset(comp2_path)\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "def comp_dataset(tags):\n",
    "    comp1_path = 'data/comp1.words'\n",
    "    comp2_path = 'data/comp2.words'\n",
    "    \n",
    "    comp_dataset = preprocess.Dataset(comp1_path, labeled=False, tags=tags)\n",
    "#     comp_dataset = preprocess.Dataset(comp2_path, labeled=False, tags=tags)\n",
    "    return comp_dataset\n",
    "\n",
    "train_dataset, val_dataset = load_datasets()\n",
    "# comp_dataset = comp_dataset(train_dataset.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_groups: 35 | total_feats: 42783\n"
     ]
    }
   ],
   "source": [
    "def create_feats(prints=True):\n",
    "    group_thresholds = {\n",
    "        # -------------------------------- feature --------------------- | -- Threshold --\n",
    "        lambda t2, t1, w, i, t: tuple([w[i].lower(), t]):                         0,     # mandatory feature f100\n",
    "        lambda t2, t1, w, i, t: tuple([w[i][-4:].lower(), t]):                    5,     # mandatory feature f101\n",
    "        lambda t2, t1, w, i, t: tuple([w[i][-3:].lower(), t]):                    5,     # mandatory feature f101\n",
    "        lambda t2, t1, w, i, t: tuple([w[i][-2:].lower(), t]):                    5,     # mandatory feature f101\n",
    "        lambda t2, t1, w, i, t: tuple([w[i][-1:].lower(), t]):                    5,     # mandatory feature f101\n",
    "        lambda t2, t1, w, i, t: tuple([w[i][:4].lower(), t]):                     5,     # mandatory feature f102\n",
    "        lambda t2, t1, w, i, t: tuple([w[i][:3].lower(), t]):                     5,     # mandatory feature f102\n",
    "        lambda t2, t1, w, i, t: tuple([w[i][:2].lower(), t]):                     5,     # mandatory feature f102\n",
    "        lambda t2, t1, w, i, t: tuple([w[i][:1].lower(), t]):                     5,     # mandatory feature f102\n",
    "        lambda t2, t1, w, i, t: tuple([t2, t1, t]):                               1,     # mandatory feature f103\n",
    "        lambda t2, t1, w, i, t: tuple([t1, t]):                                   1,     # mandatory feature f104\n",
    "        lambda t2, t1, w, i, t: tuple([t]):                                       1,     # mandatory feature f105\n",
    "        lambda t2, t1, w, i, t: tuple([w[i].islower(), t]):                       1,     # mandatory feature has_uppercase\n",
    "        lambda t2, t1, w, i, t: tuple([any(char.isdigit() for char in w[i]), t]): 1,     # mandatory feature has_digits\n",
    "        lambda t2, t1, w, i, t: tuple([w[i-1].lower(), t]):                       20,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i+1].lower(), t]):                       20,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i+1][:3].lower(), t]):                   20,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i-1][:3].lower(), t]):                   20,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i+1][:2].lower(), t]):                   20,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i-1][:2].lower(), t]):                   20,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i+1][-3:].lower(), t]):                  20,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i-1][-3:].lower(), t]):                  20,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i+1][-2:].lower(), t]):                  20,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i-1][-2:].lower(), t]):                  20,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i].isalnum(), t]):                       1,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i].isalpha(), t]):                       1,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i].isascii(), t]):                       1,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i].isdecimal(), t]):                     1,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i].isdigit(), t]):                       1,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i].isnumeric(), t]):                     1,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i].istitle(), t]):                       1,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i].isupper(), t]):                       1,\n",
    "        lambda t2, t1, w, i, t: tuple([len(w[i]) == 1, t]):                       1,\n",
    "        lambda t2, t1, w, i, t: tuple([len(w[i]) == 2, t]):                       1,\n",
    "        lambda t2, t1, w, i, t: tuple([len(w[i]) == 3, t]):                       1,\n",
    "    }\n",
    "\n",
    "    tic = time.time()\n",
    "    feature_vector = features.create_feature_vector(dataset=train_dataset,\n",
    "                                                    group_thresholds=group_thresholds,\n",
    "                                                    pruning=True,\n",
    "                                                    get_stats=False,\n",
    "                                                    assertions=True,\n",
    "                                                    calls_counter=False)\n",
    "\n",
    "    if prints:\n",
    "        print('feature_vector creation time:', time.time() - tic, 'sec\\n')\n",
    "        for feat in feature_vector.feats:\n",
    "            print('feat_group:', feat, '| feats:', len(feat))\n",
    "    print('feat_groups:', len(feature_vector.feats), '| total_feats:', len(feature_vector))\n",
    "    return feature_vector\n",
    "\n",
    "feature_vector = create_feats(prints=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w0_uniform_0_1_centered_normalized():\n",
    "    w0 = np.random.rand(len(feature_vector))\n",
    "    w0 -= w0.mean()\n",
    "    w0 /= w0.std()\n",
    "    return w0\n",
    "\n",
    "def w0_uniform_0_1_normalized():\n",
    "    w0 = np.random.rand(len(feature_vector))\n",
    "    w0 /= w0.std()\n",
    "    return w0\n",
    "\n",
    "def w0_uniform_0_1_centered():\n",
    "    w0 = np.random.rand(len(feature_vector))\n",
    "    w0 -= w0.mean()\n",
    "    return w0\n",
    "\n",
    "def w0_uniform_0_1():\n",
    "    return np.random.rand(len(feature_vector))\n",
    "\n",
    "def w0_xavier():\n",
    "    return np.random.randn(len(feature_vector))*np.sqrt(1/len(feature_vector))\n",
    "\n",
    "def w0_zero():\n",
    "    return np.zeros(len(feature_vector)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model version: 11\n",
      "epochs: 54\n",
      "train_time: 319.899\n",
      "\n",
      "last train_loss: 4.146261\n",
      "last val_loss: 4.894692\n",
      "last train_score: 0.000000\n",
      "last val_score: 0.000000\n",
      "best val_score: 0.0000 at epoch 54\n"
     ]
    }
   ],
   "source": [
    "model = classifier.load_model(from_file='checkpoint_V11_E054_SEED42.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_w0 = w0_uniform_0_1\n",
    "versions = 3\n",
    "load = False  # load last weights, log and feature_vector into model\n",
    "\n",
    "train = True  # perform a training session\n",
    "train_save = True  # save model after each training epoch, if False model will need to be saved manually\n",
    "beam = 1  # viterbi beam size for model evaluation during training\n",
    "train_aprox = 0  # aproximate train_score with first train_aprox train samples\n",
    "val_aprox = 50  # aproximate val_score with first val_aprox train samples \n",
    "# weight_decay = 0.0  # lamda regularization parameter\n",
    "# batch_size = 256  # batch_size for loader\n",
    "# epochs = 20  # training epochs\n",
    "tqdm_bar = False  # display tqdm progress bars\n",
    "\n",
    "experiments = [\n",
    "    {'weight_decay': 0.0, 'batch_size': 256, 'epochs': 15},\n",
    "    {'weight_decay': 1e-4, 'batch_size': 256, 'epochs': 15},\n",
    "    {'weight_decay': 1e-2, 'batch_size': 256, 'epochs': 15},\n",
    "    {'weight_decay': 1e-1, 'batch_size': 256, 'epochs': 15},\n",
    "    {'weight_decay': 1, 'batch_size': 256, 'epochs': 15},\n",
    "    {'weight_decay': 10, 'batch_size': 256, 'epochs': 15},\n",
    "    {'weight_decay': 100, 'batch_size': 256, 'epochs': 15},\n",
    "]\n",
    "\n",
    "for i, experiment in enumerate(experiments):\n",
    "    model = classifier.Model(version=versions + i + 1,\n",
    "                             w0=init_w0(),\n",
    "                             tags=train_dataset.tags,\n",
    "                             inference=classifier.viterbi,\n",
    "                             feature_vector=feature_vector,\n",
    "                             score_func=metrics.accuracy,\n",
    "                             models_path=models_path,\n",
    "                             save=False)\n",
    "\n",
    "    v_min, f_min, d_min = model.train(epochs=experiment['epochs'],\n",
    "                                      train_dataset=train_dataset,\n",
    "                                      val_dataset=val_dataset,\n",
    "                                      batch_size=experiment['batch_size'],\n",
    "                                      weight_decay=experiment['weight_decay'],\n",
    "                                      save=train_save,\n",
    "                                      tqdm_bar=tqdm_bar,\n",
    "                                      beam=beam,\n",
    "                                      train_aprox=train_aprox,\n",
    "                                      val_aprox=val_aprox)\n",
    "    description = f\"{inspect.getsource(init_w0).split()[1].split('(')[0]}, \" + \\\n",
    "                  f\"{len(feature_vector.feats)} feat_groups, \" + \\\n",
    "                  f\"{len(feature_vector)} total_feats\"\n",
    "\n",
    "    logger.log(model, init_w0, create_feats, load_datasets, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leadboard = logger.leadboard(col='')\n",
    "display(leadboard)\n",
    "# print(leadboard['init'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 145/1000 [03:26<19:33,  1.37s/it, acc=0.93] "
     ]
    }
   ],
   "source": [
    "aprox_num = 1000  # max samples to aproximate score\n",
    "display_all = False\n",
    "beam_stats = {}\n",
    "\n",
    "for predict_beam in [1]:  # viterbi beam size\n",
    "    beam_stats[predict_beam] = {}\n",
    "    beam_stats[predict_beam]['pred_tags'] = []\n",
    "    beam_stats[predict_beam]['true_tags'] = []\n",
    "    pbar = tqdm(val_dataset.sentences[:aprox_num])\n",
    "    for sentence in pbar:\n",
    "        preds = model(sentence[0], predict_beam)\n",
    "        beam_stats[predict_beam]['pred_tags'].append(preds)\n",
    "        beam_stats[predict_beam]['true_tags'].append(sentence[1])\n",
    "\n",
    "        if display_all:\n",
    "            display(pd.DataFrame((sentence[0], sentence[1], preds), index=('words', 'tags', 'preds')))\n",
    "        pbar.set_postfix(acc=model.score_func(beam_stats[predict_beam]['pred_tags'],\n",
    "                                              beam_stats[predict_beam]['true_tags']),\n",
    "                         refresh=False)\n",
    "\n",
    "    beam_stats[predict_beam]['matrix'], beam_stats[predict_beam]['worst'] = metrics.confusion_matrix(train_dataset.tags,\n",
    "                                                                                                     beam_stats[predict_beam]['pred_tags'],\n",
    "                                                                                                     beam_stats[predict_beam]['true_tags'])\n",
    "#     display(worst)\n",
    "model.beam_stats = beam_stats\n",
    "model.save(epoch=True, best=True)\n",
    "# model.val_predictions = pred_tags\n",
    "# model.val_sentences = val_dataset.sentences\n",
    "# model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [29:43<00:00,  1.78s/it, acc=0.906]\n",
      "100%|██████████| 1000/1000 [57:03<00:00,  3.42s/it, acc=0.907] \n",
      "100%|██████████| 1000/1000 [1:25:07<00:00,  5.11s/it, acc=0.907]\n"
     ]
    }
   ],
   "source": [
    "aprox_num = 1000  # max samples to aproximate score\n",
    "display_all = False\n",
    "beam_stats = {}\n",
    "\n",
    "for predict_beam in [1, 2, 3]:\n",
    "    beam_stats[predict_beam] = {}\n",
    "    beam_stats[predict_beam]['pred_tags'] = []\n",
    "    beam_stats[predict_beam]['true_tags'] = []\n",
    "    pbar = tqdm(val_dataset.sentences[:aprox_num])\n",
    "    for sentence in pbar:\n",
    "        preds = model(sentence[0], predict_beam)\n",
    "        beam_stats[predict_beam]['pred_tags'].append(preds)\n",
    "        beam_stats[predict_beam]['true_tags'].append(sentence[1])\n",
    "\n",
    "        if display_all:\n",
    "            display(pd.DataFrame((sentence[0], sentence[1], preds), index=('words', 'tags', 'preds')))\n",
    "        pbar.set_postfix(acc=model.score_func(beam_stats[predict_beam]['pred_tags'],\n",
    "                                              beam_stats[predict_beam]['true_tags']),\n",
    "                         refresh=False)\n",
    "\n",
    "    beam_stats[predict_beam]['matrix'], beam_stats[predict_beam]['worst'] = metrics.confusion_matrix(train_dataset.tags,\n",
    "                                                                                                     beam_stats[predict_beam]['pred_tags'],\n",
    "                                                                                                     beam_stats[predict_beam]['true_tags'])\n",
    "#     display(worst)\n",
    "\n",
    "# model.val_predictions = pred_tags\n",
    "# model.val_sentences = val_dataset.sentences\n",
    "# model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.beam_stats = beam_stats\n",
    "model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(matrix1)\n",
    "display(matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(worst1)\n",
    "display(worst2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code that may be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = {}\n",
    "# for sentence in train_dataset.sentences:\n",
    "#     sentences[len(sentence[0])] = sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# sample_sentence = (['Terms', 'were', \"n't\", 'disclosed', '.'],  # sentence words\n",
    "#                    ['NNS',   'VBD',  'RB',  'VBN',       '.'])  # sentence true tags\n",
    "# predict_beam = 100  # viterbi beam size\n",
    "\n",
    "# tags, bp_pi = viterbi(model, sample_sentence[0], beam=predict_beam)\n",
    "# print('sentence ', sample_sentence[0])\n",
    "# print('true tags', sample_sentence[1])\n",
    "# print('pred tags', tags)\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3802186.0\n",
      "760.4372\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32871</th>\n",
       "      <td>16939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42333</th>\n",
       "      <td>16939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42378</th>\n",
       "      <td>16939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42289</th>\n",
       "      <td>16939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42423</th>\n",
       "      <td>16939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7471</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7472</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13388</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7476</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8908</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42783 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        feat\n",
       "32871  16939\n",
       "42333  16939\n",
       "42378  16939\n",
       "42289  16939\n",
       "42423  16939\n",
       "...      ...\n",
       "7471       1\n",
       "7472       1\n",
       "13388      1\n",
       "7476       1\n",
       "8908       1\n",
       "\n",
       "[42783 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.2 s, sys: 15.6 ms, total: 28.3 s\n",
      "Wall time: 28.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sum_vec = np.zeros(len(feature_vector)).astype(np.float32)\n",
    "sum_inds = 0\n",
    "for t2, t1, w, i, t in train_dataset:\n",
    "    vec = feature_vector(t2, t1, w, i, t, fmt='vec')\n",
    "    sum_vec += vec\n",
    "    sum_inds += (vec).sum()\n",
    "\n",
    "df = pd.DataFrame(sum_vec, columns=['feat']).astype({'feat': int}).sort_values('feat', ascending=False)\n",
    "print(sum_vec.sum())\n",
    "print(sum_inds/len(train_dataset.sentences))\n",
    "display(df.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17008</th>\n",
       "      <td>6199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19968</th>\n",
       "      <td>6199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42480</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24990</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19758</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20012</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26421</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42299</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42179</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32881</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22556</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42388</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33000</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32927</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17056</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42433</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42667</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18812</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42239</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14771</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42611</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42735</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42343</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42554</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42655</th>\n",
       "      <td>5104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42606</th>\n",
       "      <td>4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42383</th>\n",
       "      <td>4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42174</th>\n",
       "      <td>4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32921</th>\n",
       "      <td>4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42294</th>\n",
       "      <td>4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42549</th>\n",
       "      <td>4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32995</th>\n",
       "      <td>4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42474</th>\n",
       "      <td>4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42662</th>\n",
       "      <td>4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32876</th>\n",
       "      <td>4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42234</th>\n",
       "      <td>4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42428</th>\n",
       "      <td>4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42728</th>\n",
       "      <td>4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42338</th>\n",
       "      <td>4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31974</th>\n",
       "      <td>4934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18787</th>\n",
       "      <td>4914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14742</th>\n",
       "      <td>4914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19740</th>\n",
       "      <td>4914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24962</th>\n",
       "      <td>4914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19988</th>\n",
       "      <td>4914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17030</th>\n",
       "      <td>4914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22530</th>\n",
       "      <td>4914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       feat\n",
       "0      6199\n",
       "17008  6199\n",
       "19968  6199\n",
       "42480  6044\n",
       "24990  6044\n",
       "19758  6044\n",
       "20012  6044\n",
       "56     6044\n",
       "26421  6044\n",
       "42299  6044\n",
       "42179  6044\n",
       "32881  6044\n",
       "22556  6044\n",
       "42388  6044\n",
       "33000  6044\n",
       "32927  6044\n",
       "17056  6044\n",
       "42433  6044\n",
       "42667  6044\n",
       "18812  6044\n",
       "42239  6044\n",
       "14771  6044\n",
       "42611  6044\n",
       "42735  6044\n",
       "42343  6044\n",
       "42554  6044\n",
       "42655  5104\n",
       "42606  4962\n",
       "42383  4962\n",
       "42174  4962\n",
       "32921  4962\n",
       "42294  4962\n",
       "42549  4962\n",
       "32995  4962\n",
       "42474  4962\n",
       "42662  4962\n",
       "32876  4962\n",
       "42234  4962\n",
       "42428  4962\n",
       "42728  4962\n",
       "42338  4962\n",
       "31974  4934\n",
       "18787  4914\n",
       "14742  4914\n",
       "19740  4914\n",
       "24962  4914\n",
       "23     4914\n",
       "19988  4914\n",
       "17030  4914\n",
       "22530  4914"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.loc[0:100].head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureGroup(tuple([w[i].lower(), t]))\n",
      "('low-density', 'NN')\n"
     ]
    }
   ],
   "source": [
    "feat, key = feature_vector.invert_feat(4914)  # 41453 22811\n",
    "print(feat)\n",
    "print(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test run train_dataset\n",
    "# tic = time.time()\n",
    "# for t2, t1, w, i, t in train_dataset:\n",
    "#     feat_vec_t = feature_vector(t2, t1, w, i, t, fmt='vec')\n",
    "# print('fmt=vec: {:.3f} sec'.format(time.time() - tic))\n",
    "\n",
    "# tic = time.time()\n",
    "# for t2, t1, w, i, t in train_dataset:\n",
    "#     feat_list_t = feature_vector(t2, t1, w, i, t, fmt='list')\n",
    "# print('fmt=list: {:.3f} sec'.format(time.time() - tic))\n",
    "\n",
    "# tic = time.time()\n",
    "# for t2, t1, w, i, t in train_dataset:\n",
    "#     feat_vec_t, feat_list_t = feature_vector(t2, t1, w, i, t, fmt='both')\n",
    "# print('fmt=vec+list: {:.3f} sec'.format(time.time() - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tag in train1_statistics.words_per_tag:\n",
    "#     if len(train1_statistics.words_per_tag[tag]) < 10:\n",
    "#         print('{:5} tf: {:5d} unique_count: {:4d} words: {}'.format(tag, train1_statistics.tags_count[tag], len(train1_statistics.words_per_tag[tag]),\n",
    "#                                                                     train1_statistics.words_per_tag[tag]))\n",
    "#     else:\n",
    "#         print('{:5} tf: {:5d} unique_count: {:4d}'.format(tag, train1_statistics.tags_count[tag], len(train1_statistics.words_per_tag[tag])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = []\n",
    "\n",
    "# # one-to-one features\n",
    "# for word in strange_words:\n",
    "#     features.append(Feature(f'w[i] == \"{word}\"', t=train1_model.tags_per_word[word][0]))\n",
    "#     print(word, train1_model.WordCount[word], train1_model.TagsPerWord[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_hw1",
   "language": "python",
   "name": "nlp_hw1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
