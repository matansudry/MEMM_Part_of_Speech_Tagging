{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from utils import preprocess, features, classifier, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp1_path = 'data/comp1.words'\n",
    "comp2_path = 'data/comp2.words'\n",
    "test1_path = 'data/test1.wtag'\n",
    "train1_path = 'data/train1.wtag'\n",
    "train2_path = 'data/train2.wtag'\n",
    "\n",
    "train_dataset = preprocess.Dataset(train1_path)\n",
    "val_dataset = preprocess.Dataset(test1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features, only used when load = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_vector creation time: 3.0340416431427 sec\n",
      "\n",
      "feat_group: FeatureGroup(tuple([w[i].lower(), t])) | feats: 14719\n",
      "feat_group: FeatureGroup(tuple([w[i][-4:].lower(), t])) | feats: 2289\n",
      "feat_group: FeatureGroup(tuple([w[i][-3:].lower(), t])) | feats: 1757\n",
      "feat_group: FeatureGroup(tuple([w[i][-2:].lower(), t])) | feats: 955\n",
      "feat_group: FeatureGroup(tuple([w[i][-1:].lower(), t])) | feats: 248\n",
      "feat_group: FeatureGroup(tuple([w[i][:4].lower(), t])) | feats: 2540\n",
      "feat_group: FeatureGroup(tuple([w[i][:3].lower(), t])) | feats: 2431\n",
      "feat_group: FeatureGroup(tuple([w[i][:2].lower(), t])) | feats: 1432\n",
      "feat_group: FeatureGroup(tuple([w[i][:1].lower(), t])) | feats: 392\n",
      "feat_group: FeatureGroup(tuple([t2, t1, t])) | feats: 5192\n",
      "feat_group: FeatureGroup(tuple([t1, t])) | feats: 908\n",
      "feat_group: FeatureGroup(tuple([t])) | feats: 44\n",
      "feat_group: FeatureGroup(tuple([w[i].islower(), t])) | feats: 75\n",
      "feat_group: FeatureGroup(tuple([any(char.isdigit() for char in w[i]), t])) | feats: 49\n",
      "feat_group: FeatureGroup(tuple([w[i-1].lower(), t])) | feats: 2641\n",
      "feat_group: FeatureGroup(tuple([w[i+1].lower(), t])) | feats: 2631\n",
      "feat_group: FeatureGroup(tuple([w[i+1][:3].lower(), t])) | feats: 869\n",
      "feat_group: FeatureGroup(tuple([w[i-1][:3].lower(), t])) | feats: 938\n",
      "feat_group: FeatureGroup(tuple([w[i+1][:2].lower(), t])) | feats: 1081\n",
      "feat_group: FeatureGroup(tuple([w[i-1][:2].lower(), t])) | feats: 1108\n",
      "feat_group: FeatureGroup(tuple([w[i+1][-3:].lower(), t])) | feats: 962\n",
      "feat_group: FeatureGroup(tuple([w[i-1][-3:].lower(), t])) | feats: 971\n",
      "feat_group: FeatureGroup(tuple([w[i+1][-2:].lower(), t])) | feats: 975\n",
      "feat_group: FeatureGroup(tuple([w[i-1][-2:].lower(), t])) | feats: 996\n",
      "feat_group: FeatureGroup(tuple([w[i].isalnum(), t])) | feats: 60\n",
      "feat_group: FeatureGroup(tuple([w[i].isalpha(), t])) | feats: 60\n",
      "feat_group: FeatureGroup(tuple([w[i].isascii(), t])) | feats: 44\n",
      "feat_group: FeatureGroup(tuple([w[i].isdecimal(), t])) | feats: 45\n",
      "feat_group: FeatureGroup(tuple([w[i].isdigit(), t])) | feats: 45\n",
      "feat_group: FeatureGroup(tuple([w[i].isnumeric(), t])) | feats: 45\n",
      "feat_group: FeatureGroup(tuple([w[i].istitle(), t])) | feats: 76\n",
      "feat_group: FeatureGroup(tuple([w[i].isupper(), t])) | feats: 56\n",
      "feat_group: FeatureGroup(tuple([len(w[i]) == 1, t])) | feats: 56\n",
      "feat_group: FeatureGroup(tuple([len(w[i]) == 2, t])) | feats: 66\n",
      "feat_group: FeatureGroup(tuple([len(w[i]) == 3, t])) | feats: 69\n",
      "feat_groups: 35 | total_feats: 46825\n"
     ]
    }
   ],
   "source": [
    "group_thresholds = {\n",
    "    # ---------- feature -------------------------------------------- | -- Threshold --\n",
    "    lambda t2, t1, w, i, t: tuple([w[i].lower(), t]):                         0,  # mandatory feature f100\n",
    "    lambda t2, t1, w, i, t: tuple([w[i][-4:].lower(), t]):                    5,     # mandatory feature f101\n",
    "    lambda t2, t1, w, i, t: tuple([w[i][-3:].lower(), t]):                    5,     # mandatory feature f101\n",
    "    lambda t2, t1, w, i, t: tuple([w[i][-2:].lower(), t]):                    5,     # mandatory feature f101\n",
    "    lambda t2, t1, w, i, t: tuple([w[i][-1:].lower(), t]):                    5,     # mandatory feature f101\n",
    "    lambda t2, t1, w, i, t: tuple([w[i][:4].lower(), t]):                     5,     # mandatory feature f102\n",
    "    lambda t2, t1, w, i, t: tuple([w[i][:3].lower(), t]):                     5,     # mandatory feature f102\n",
    "    lambda t2, t1, w, i, t: tuple([w[i][:2].lower(), t]):                     5,     # mandatory feature f102\n",
    "    lambda t2, t1, w, i, t: tuple([w[i][:1].lower(), t]):                     5,     # mandatory feature f102\n",
    "    lambda t2, t1, w, i, t: tuple([t2, t1, t]):                               1,     # mandatory feature f103\n",
    "    lambda t2, t1, w, i, t: tuple([t1, t]):                                   1,     # mandatory feature f104\n",
    "    lambda t2, t1, w, i, t: tuple([t]):                                       1,     # mandatory feature f105\n",
    "    lambda t2, t1, w, i, t: tuple([w[i].islower(), t]):                       1,     # mandatory feature has uppercase\n",
    "    lambda t2, t1, w, i, t: tuple([any(char.isdigit() for char in w[i]), t]): 1,     # mandatory feature has digits\n",
    "    lambda t2, t1, w, i, t: tuple([w[i-1].lower(), t]):                       5,\n",
    "    lambda t2, t1, w, i, t: tuple([w[i+1].lower(), t]):                       5,\n",
    "    lambda t2, t1, w, i, t: tuple([w[i+1][:3].lower(), t]):                   20,\n",
    "    lambda t2, t1, w, i, t: tuple([w[i-1][:3].lower(), t]):                   20,\n",
    "    lambda t2, t1, w, i, t: tuple([w[i+1][:2].lower(), t]):                   20,\n",
    "    lambda t2, t1, w, i, t: tuple([w[i-1][:2].lower(), t]):                   20,\n",
    "    lambda t2, t1, w, i, t: tuple([w[i+1][-3:].lower(), t]):                  20,\n",
    "    lambda t2, t1, w, i, t: tuple([w[i-1][-3:].lower(), t]):                  20,\n",
    "    lambda t2, t1, w, i, t: tuple([w[i+1][-2:].lower(), t]):                  20,\n",
    "    lambda t2, t1, w, i, t: tuple([w[i-1][-2:].lower(), t]):                  20,\n",
    "    lambda t2, t1, w, i, t: tuple([w[i].isalnum(), t]):                       1,\n",
    "    lambda t2, t1, w, i, t: tuple([w[i].isalpha(), t]):                       1,\n",
    "    lambda t2, t1, w, i, t: tuple([w[i].isascii(), t]):                       1,\n",
    "    lambda t2, t1, w, i, t: tuple([w[i].isdecimal(), t]):                     1,\n",
    "    lambda t2, t1, w, i, t: tuple([w[i].isdigit(), t]):                       1,\n",
    "    lambda t2, t1, w, i, t: tuple([w[i].isnumeric(), t]):                     1,\n",
    "    lambda t2, t1, w, i, t: tuple([w[i].istitle(), t]):                       1,\n",
    "    lambda t2, t1, w, i, t: tuple([w[i].isupper(), t]):                       1,\n",
    "    lambda t2, t1, w, i, t: tuple([len(w[i]) == 1, t]):                       1,\n",
    "    lambda t2, t1, w, i, t: tuple([len(w[i]) == 2, t]):                       1,\n",
    "    lambda t2, t1, w, i, t: tuple([len(w[i]) == 3, t]):                       1,\n",
    "}\n",
    "\n",
    "tic = time.time()\n",
    "feature_vector, feature_groups_stats = features.create_feature_vector(dataset=train_dataset,\n",
    "                                                group_thresholds=group_thresholds,\n",
    "                                                pruning=True,\n",
    "                                                get_stats=True,\n",
    "                                                assertions=True,\n",
    "                                                calls_counter=True)\n",
    "\n",
    "print('feature_vector creation time:', time.time() - tic, 'sec\\n')\n",
    "for feat in feature_vector.feats:\n",
    "    print('feat_group:', feat, '| feats:', len(feat))\n",
    "print('feat_groups:', len(feature_vector.feats), '| total_feats:', len(feature_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.random.rand(len(feature_vector)).astype(np.float32)  # init weights\n",
    "# w0 = np.zeros(len(feature_vector)).astype(np.float32)  # init weights to zero\n",
    "# # perform data-aware initialization\n",
    "# for h in train_dataset:\n",
    "#     w0 += feature_vector(*h, fmt='vec')\n",
    "# w0 /= len(train_dataset.sentences)\n",
    "\n",
    "seed = 42  # seed for batch shuffle for loader\n",
    "models_path = 'models'  # folder of models\n",
    "load = False  # load last weights, log and feature_vector into model\n",
    "\n",
    "train = True  # perform a training session\n",
    "train_save = True  # save model after each training epoch, if False model will need to be saved manually\n",
    "beam = 1  # viterbi beam size for model evaluation during training\n",
    "train_aprox = 0  # aproximate train_score with first train_aprox train samples\n",
    "val_aprox = 50  # aproximate val_score with first val_aprox train samples \n",
    "weight_decay = 0.0  # lamda regularization parameter\n",
    "batch_size = 256  # batch_size for loader\n",
    "epochs = 20  # training epochs\n",
    "tqdm_bar = False  # display tqdm progress bars\n",
    "# generated model version ID\n",
    "version = f\"data-aware-init_weight_decay-{weight_decay}_batch_size-{batch_size}_max_epochs-{epochs}_feats-{len(feature_vector.feats)}\"\n",
    "\n",
    "model = classifier.Model(version=version,\n",
    "                         w0=w0,\n",
    "                         tags=train_dataset.tags,\n",
    "                         inference=classifier.viterbi,\n",
    "                         feature_vector=feature_vector,\n",
    "                         seed=seed,\n",
    "                         score_func=metrics.accuracy,\n",
    "                         models_path=models_path,\n",
    "                         save=False)\n",
    "\n",
    "if load:\n",
    "    model.load(weights=True, feature_vector=True, log=True, epoch=-1, prints=True)\n",
    "\n",
    "if train:\n",
    "    v_min, f_min, d_min = model.train(epochs=epochs,\n",
    "                                      train_dataset=train_dataset,\n",
    "                                      val_dataset=val_dataset,\n",
    "                                      batch_size=batch_size,\n",
    "                                      weight_decay=weight_decay,\n",
    "                                      save=train_save,\n",
    "                                      tqdm_bar=tqdm_bar,\n",
    "                                      beam=beam,\n",
    "                                      train_aprox=train_aprox,\n",
    "                                      val_aprox=val_aprox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manual model evaluation on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aprox_num = 100  # max samples to aproximate score\n",
    "predict_beam = 2  # viterbi beam size\n",
    "dataset = val_dataset  # dataset to evaluate\n",
    "display_all = True\n",
    "\n",
    "pred_tags = []\n",
    "true_tags = []\n",
    "pbar = tqdm(dataset.sentences[:aprox_num])\n",
    "for sentence in pbar:\n",
    "    preds = model(sentence[0], predict_beam)\n",
    "    pred_tags.append(preds)\n",
    "    true_tags.append(sentence[1])\n",
    "    \n",
    "    if display_all:\n",
    "        display(pd.DataFrame((sentence[0], sentence[1], preds), index=('words', 'tags', 'preds')))\n",
    "    pbar.set_postfix(acc=model.score_func(pred_tags, true_tags), refresh=False)\n",
    "    \n",
    "matrix, worst = metrics.confusion_matrix(train_dataset.tags, pred_tags, true_tags)\n",
    "display(worst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3841466.0\n",
      "768.2932\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46331</th>\n",
       "      <td>16939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46420</th>\n",
       "      <td>16939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46465</th>\n",
       "      <td>16939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46375</th>\n",
       "      <td>16939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32871</th>\n",
       "      <td>16939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7544</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13932</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7548</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7550</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11381</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46825 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        feat\n",
       "46331  16939\n",
       "46420  16939\n",
       "46465  16939\n",
       "46375  16939\n",
       "32871  16939\n",
       "...      ...\n",
       "7544       1\n",
       "13932      1\n",
       "7548       1\n",
       "7550       1\n",
       "11381      1\n",
       "\n",
       "[46825 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.3 s, sys: 0 ns, total: 13.3 s\n",
      "Wall time: 13.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sum_vec = np.zeros(len(feature_vector)).astype(np.float32)\n",
    "sum_inds = 0\n",
    "for t2, t1, w, i, t in train_dataset:\n",
    "    try:\n",
    "        vec = feature_vector(t2, t1, w, i, t, fmt='vec')\n",
    "    except Exception as e:\n",
    "        print(t2, t1, w, i, t)\n",
    "        raise e\n",
    "    sum_vec += vec\n",
    "    sum_inds += (vec).sum()\n",
    "\n",
    "df = pd.DataFrame(sum_vec, columns=['feat']).astype({'feat': int}).sort_values('feat', ascending=False)\n",
    "print(sum_vec.sum())\n",
    "print(sum_inds/len(train_dataset.sentences))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the', 'DT')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2, t1, w, i, t = '*', '*', ['The', 'Treasury', 'is', 'still', 'working', 'out', 'the', 'details', 'with', 'bank', 'trade', 'associations', 'and', 'the', 'other', 'government', 'agencies', 'that', 'have', 'a', 'hand', 'in', 'fighting', 'money', 'laundering', '.'], 0, 'DT'\n",
    "# foo = lambda t2, t1, w, i, t: tuple([w[i][-3:].lower(), t])\n",
    "feature_vector.feats[2].foo(t2, t1, w, i, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureGroup(tuple([w[i][-3:].lower(), t]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vector.feats[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureGroup(tuple([w[i].isnumeric(), t]))\n",
      "(False, 'NN')\n",
      "16939\n"
     ]
    }
   ],
   "source": [
    "feat, key = feature_vector.invert_feat(33249)  # 41453 22811\n",
    "print(feat)\n",
    "print(key)\n",
    "print(feat.hash_calls[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureGroup(tuple([w[i].lower(), t])) 3\n",
      "FeatureGroup(tuple([w[i][-4:].lower(), t])) 3\n",
      "FeatureGroup(tuple([w[i][-3:].lower(), t])) 0\n",
      "FeatureGroup(tuple([w[i][-2:].lower(), t])) 0\n",
      "FeatureGroup(tuple([w[i][-1:].lower(), t])) 0\n",
      "FeatureGroup(tuple([w[i][:4].lower(), t])) 0\n",
      "FeatureGroup(tuple([w[i][:3].lower(), t])) 0\n",
      "FeatureGroup(tuple([w[i][:2].lower(), t])) 0\n",
      "FeatureGroup(tuple([w[i][:1].lower(), t])) 0\n",
      "FeatureGroup(tuple([t2, t1, t])) 0\n",
      "FeatureGroup(tuple([t1, t])) 0\n",
      "FeatureGroup(tuple([w[i].islower(), t])) 0\n",
      "FeatureGroup(tuple([any(char.isdigit() for char in w[i]), t])) 0\n",
      "FeatureGroup(tuple([w[i].isalnum(), t])) 0\n",
      "FeatureGroup(tuple([w[i].isalpha(), t])) 0\n",
      "FeatureGroup(tuple([w[i].isascii(), t])) 0\n",
      "FeatureGroup(tuple([w[i].isdecimal(), t])) 0\n",
      "FeatureGroup(tuple([w[i].isdigit(), t])) 0\n",
      "FeatureGroup(tuple([w[i].isnumeric(), t])) 0\n",
      "FeatureGroup(tuple([w[i].istitle(), t])) 0\n",
      "FeatureGroup(tuple([w[i].isupper(), t])) 0\n",
      "FeatureGroup(tuple([len(w[i]) == 1, t])) 0\n",
      "FeatureGroup(tuple([len(w[i]) == 2, t])) 0\n",
      "FeatureGroup(tuple([len(w[i]) == 3, t])) 0\n"
     ]
    }
   ],
   "source": [
    "for feat in feature_vector.feats:\n",
    "    print(feat, feat.calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = lambda t2, t1, w, i, t: tuple([w[i][-3:].lower(), t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 'DT')\n",
      "('ech', 'NNP')\n",
      "('man', 'NN')\n",
      "('aid', 'VBD')\n",
      "('the', 'DT')\n",
      "('ent', 'NN')\n",
      "('lls', 'VBZ')\n",
      "('for', 'IN')\n",
      "('hst', 'NNP')\n",
      "('to', 'TO')\n",
      "('ote', 'VB')\n",
      "('tpa', 'NNP')\n",
      "('for', 'IN')\n",
      "('art', 'NN')\n",
      "('nts', 'NNS')\n",
      "('and', 'CC')\n",
      "('ase', 'NNP')\n",
      "('for', 'IN')\n",
      "('her', 'JJ')\n",
      "('ing', 'JJ')\n",
      "('ses', 'NNS')\n",
      "('.', '.')\n"
     ]
    }
   ],
   "source": [
    "sentence = train_dataset.sentences[25]\n",
    "\n",
    "t1, t = '*', '*'\n",
    "for i in range(len(sentence[0])):\n",
    "    t2, t1, t = t1, t, sentence[1][i]\n",
    "#     print(feat.foo(t2, t1, sentence[0], i, t))\n",
    "    print(foo(t2, t1, sentence[0], i, t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['A',\n",
       "  'Genentech',\n",
       "  'spokeswoman',\n",
       "  'said',\n",
       "  'the',\n",
       "  'agreement',\n",
       "  'calls',\n",
       "  'for',\n",
       "  'Hoechst',\n",
       "  'to',\n",
       "  'promote',\n",
       "  'TPA',\n",
       "  'for',\n",
       "  'heart',\n",
       "  'patients',\n",
       "  'and',\n",
       "  'streptokinase',\n",
       "  'for',\n",
       "  'other',\n",
       "  'clot-reducing',\n",
       "  'purposes',\n",
       "  '.'],\n",
       " ['DT',\n",
       "  'NNP',\n",
       "  'NN',\n",
       "  'VBD',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'VBZ',\n",
       "  'IN',\n",
       "  'NNP',\n",
       "  'TO',\n",
       "  'VB',\n",
       "  'NNP',\n",
       "  'IN',\n",
       "  'NN',\n",
       "  'NNS',\n",
       "  'CC',\n",
       "  'NNP',\n",
       "  'IN',\n",
       "  'JJ',\n",
       "  'JJ',\n",
       "  'NNS',\n",
       "  '.'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.2398, 0.0086, 0.1972, ..., 0.    , 0.    , 0.    ], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0 = np.zeros(len(feature_vector)).astype(np.float32)  # init weights to zero\n",
    "# perform data-aware initialization\n",
    "for h in train_dataset:\n",
    "    w0 += feature_vector(*h, fmt='vec')\n",
    "w0 /= len(train_dataset.sentences)\n",
    "w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code that may be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# sample_sentence = (['Terms', 'were', \"n't\", 'disclosed', '.'],  # sentence words\n",
    "#                    ['NNS',   'VBD',  'RB',  'VBN',       '.'])  # sentence true tags\n",
    "# predict_beam = 100  # viterbi beam size\n",
    "\n",
    "# tags, bp_pi = viterbi(model, sample_sentence[0], beam=predict_beam)\n",
    "# print('sentence ', sample_sentence[0])\n",
    "# print('true tags', sample_sentence[1])\n",
    "# print('pred tags', tags)\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = {}\n",
    "# for sentence in train_dataset.sentences:\n",
    "#     sentences[len(sentence[0])] = sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat, key = feature_vector.invert_feat(33249)  # 41453 22811\n",
    "# print(feat)\n",
    "# print(key)\n",
    "# print(feat.hash_calls[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test run train_dataset\n",
    "# tic = time.time()\n",
    "# for t2, t1, w, i, t in train_dataset:\n",
    "#     feat_vec_t = feature_vector(t2, t1, w, i, t, fmt='vec')\n",
    "# print('fmt=vec: {:.3f} sec'.format(time.time() - tic))\n",
    "\n",
    "# tic = time.time()\n",
    "# for t2, t1, w, i, t in train_dataset:\n",
    "#     feat_list_t = feature_vector(t2, t1, w, i, t, fmt='list')\n",
    "# print('fmt=list: {:.3f} sec'.format(time.time() - tic))\n",
    "\n",
    "# tic = time.time()\n",
    "# for t2, t1, w, i, t in train_dataset:\n",
    "#     feat_vec_t, feat_list_t = feature_vector(t2, t1, w, i, t, fmt='both')\n",
    "# print('fmt=vec+list: {:.3f} sec'.format(time.time() - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tag in train1_statistics.words_per_tag:\n",
    "#     if len(train1_statistics.words_per_tag[tag]) < 10:\n",
    "#         print('{:5} tf: {:5d} unique_count: {:4d} words: {}'.format(tag, train1_statistics.tags_count[tag], len(train1_statistics.words_per_tag[tag]),\n",
    "#                                                                     train1_statistics.words_per_tag[tag]))\n",
    "#     else:\n",
    "#         print('{:5} tf: {:5d} unique_count: {:4d}'.format(tag, train1_statistics.tags_count[tag], len(train1_statistics.words_per_tag[tag])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = []\n",
    "\n",
    "# # one-to-one features\n",
    "# for word in strange_words:\n",
    "#     features.append(Feature(f'w[i] == \"{word}\"', t=train1_model.tags_per_word[word][0]))\n",
    "#     print(word, train1_model.WordCount[word], train1_model.TagsPerWord[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_hw1",
   "language": "python",
   "name": "nlp_hw1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
