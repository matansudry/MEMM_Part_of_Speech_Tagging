{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import inspect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from utils import preprocess, features, classifier, metrics, logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "models_path = 'models'  # folder of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger2 = logging.Logger('log/log2.csv')\n",
    "# logger2.leadboard(col='val_score', top=5)\n",
    "# logger2.init_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger1 = logging.Logger('log/log1.csv')\n",
    "# logger1.leadboard(col='val_score', top=10)\n",
    "# logger1.init_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets():\n",
    "#     train_dataset = preprocess.Dataset('data/train1.wtag')\n",
    "#     val_dataset = preprocess.Dataset('data/comp1_tagged.wtag')\n",
    "\n",
    "    train_dataset = preprocess.Dataset('data/train2.wtag')\n",
    "    val_dataset = preprocess.Dataset('data/comp2_tagged.wtag')\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "train_dataset, val_dataset = load_datasets()\n",
    "tags = train_dataset.tags.union(val_dataset.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_vector creation time: 0.16922855377197266 sec\n",
      "\n",
      "feat_group: FeatureGroup(tuple([w[i].lower(), t])) | feats: 1736\n",
      "feat_group: FeatureGroup(tuple([w[i][-4:].lower(), t])) | feats: 170\n",
      "feat_group: FeatureGroup(tuple([w[i][-3:].lower(), t])) | feats: 165\n",
      "feat_group: FeatureGroup(tuple([w[i][-2:].lower(), t])) | feats: 145\n",
      "feat_group: FeatureGroup(tuple([w[i][-1:].lower(), t])) | feats: 96\n",
      "feat_group: FeatureGroup(tuple([w[i][:4].lower(), t])) | feats: 158\n",
      "feat_group: FeatureGroup(tuple([w[i][:3].lower(), t])) | feats: 170\n",
      "feat_group: FeatureGroup(tuple([w[i][:2].lower(), t])) | feats: 200\n",
      "feat_group: FeatureGroup(tuple([w[i][:1].lower(), t])) | feats: 133\n",
      "feat_group: FeatureGroup(tuple([t2, t1, t])) | feats: 598\n",
      "feat_group: FeatureGroup(tuple([t1, t])) | feats: 216\n",
      "feat_group: FeatureGroup(tuple([t])) | feats: 31\n",
      "feat_group: FeatureGroup(tuple([w[i].islower(), t])) | feats: 43\n",
      "feat_group: FeatureGroup(tuple([any(char.isdigit() for char in w[i]), t])) | feats: 35\n",
      "feat_group: FeatureGroup(tuple([w[i-1].lower(), t])) | feats: 28\n",
      "feat_group: FeatureGroup(tuple([w[i+1].lower(), t])) | feats: 23\n",
      "feat_group: FeatureGroup(tuple([w[i+1][:3].lower(), t])) | feats: 29\n",
      "feat_group: FeatureGroup(tuple([w[i-1][:3].lower(), t])) | feats: 35\n",
      "feat_group: FeatureGroup(tuple([w[i+1][:2].lower(), t])) | feats: 44\n",
      "feat_group: FeatureGroup(tuple([w[i-1][:2].lower(), t])) | feats: 42\n",
      "feat_group: FeatureGroup(tuple([w[i+1][-3:].lower(), t])) | feats: 30\n",
      "feat_group: FeatureGroup(tuple([w[i-1][-3:].lower(), t])) | feats: 41\n",
      "feat_group: FeatureGroup(tuple([w[i+1][-2:].lower(), t])) | feats: 49\n",
      "feat_group: FeatureGroup(tuple([w[i-1][-2:].lower(), t])) | feats: 61\n",
      "feat_group: FeatureGroup(tuple([w[i].isalnum(), t])) | feats: 24\n",
      "feat_group: FeatureGroup(tuple([w[i].isalpha(), t])) | feats: 24\n",
      "feat_group: FeatureGroup(tuple([w[i].isascii(), t])) | feats: 21\n",
      "feat_group: FeatureGroup(tuple([w[i].isdecimal(), t])) | feats: 22\n",
      "feat_group: FeatureGroup(tuple([w[i].isdigit(), t])) | feats: 22\n",
      "feat_group: FeatureGroup(tuple([w[i].isnumeric(), t])) | feats: 22\n",
      "feat_group: FeatureGroup(tuple([w[i].istitle(), t])) | feats: 27\n",
      "feat_group: FeatureGroup(tuple([w[i].isupper(), t])) | feats: 22\n",
      "feat_group: FeatureGroup(tuple([len(w[i]), t])) | feats: 86\n",
      "feat_groups: 33 | total_feats: 4548\n"
     ]
    }
   ],
   "source": [
    "def create_feats(prints=True):\n",
    "    group_thresholds = {\n",
    "        # -------------------------------- feature --------------------- | -- Threshold --\n",
    "        lambda t2, t1, w, i, t: tuple([w[i].lower(), t]):                         0,     # mandatory feature f100\n",
    "        lambda t2, t1, w, i, t: tuple([w[i][-4:].lower(), t]):                    5,     # mandatory feature f101\n",
    "        lambda t2, t1, w, i, t: tuple([w[i][-3:].lower(), t]):                    5,     # mandatory feature f101\n",
    "        lambda t2, t1, w, i, t: tuple([w[i][-2:].lower(), t]):                    5,     # mandatory feature f101\n",
    "        lambda t2, t1, w, i, t: tuple([w[i][-1:].lower(), t]):                    5,     # mandatory feature f101\n",
    "        lambda t2, t1, w, i, t: tuple([w[i][:4].lower(), t]):                     5,     # mandatory feature f102\n",
    "        lambda t2, t1, w, i, t: tuple([w[i][:3].lower(), t]):                     5,     # mandatory feature f102\n",
    "        lambda t2, t1, w, i, t: tuple([w[i][:2].lower(), t]):                     5,     # mandatory feature f102\n",
    "        lambda t2, t1, w, i, t: tuple([w[i][:1].lower(), t]):                     5,     # mandatory feature f102\n",
    "        lambda t2, t1, w, i, t: tuple([t2, t1, t]):                               1,     # mandatory feature f103\n",
    "        lambda t2, t1, w, i, t: tuple([t1, t]):                                   1,     # mandatory feature f104\n",
    "        lambda t2, t1, w, i, t: tuple([t]):                                       1,     # mandatory feature f105\n",
    "        lambda t2, t1, w, i, t: tuple([w[i].islower(), t]):                       1,     # mandatory feature has_uppercase\n",
    "        lambda t2, t1, w, i, t: tuple([any(char.isdigit() for char in w[i]), t]): 1,     # mandatory feature has_digits\n",
    "        lambda t2, t1, w, i, t: tuple([w[i-1].lower(), t]):                       20,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i+1].lower(), t]):                       20,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i+1][:3].lower(), t]):                   20,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i-1][:3].lower(), t]):                   20,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i+1][:2].lower(), t]):                   20,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i-1][:2].lower(), t]):                   20,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i+1][-3:].lower(), t]):                  20,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i-1][-3:].lower(), t]):                  20,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i+1][-2:].lower(), t]):                  20,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i-1][-2:].lower(), t]):                  20,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i].isalnum(), t]):                       10,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i].isalpha(), t]):                       10,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i].isascii(), t]):                       10,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i].isdecimal(), t]):                     10,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i].isdigit(), t]):                       10,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i].isnumeric(), t]):                     10,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i].istitle(), t]):                       10,\n",
    "        lambda t2, t1, w, i, t: tuple([w[i].isupper(), t]):                       10,\n",
    "        lambda t2, t1, w, i, t: tuple([len(w[i]), t]):                            10,\n",
    "    }\n",
    "\n",
    "    tic = time.time()\n",
    "    feature_vector = features.create_feature_vector(dataset=train_dataset,\n",
    "                                                    group_thresholds=group_thresholds,\n",
    "                                                    pruning=True,\n",
    "                                                    get_stats=False,\n",
    "                                                    assertions=True,\n",
    "                                                    calls_counter=False)\n",
    "\n",
    "    if prints:\n",
    "        print('feature_vector creation time:', time.time() - tic, 'sec\\n')\n",
    "        for feat in feature_vector.feats:\n",
    "            print('feat_group:', feat, '| feats:', len(feat))\n",
    "    print('feat_groups:', len(feature_vector.feats), '| total_feats:', len(feature_vector))\n",
    "    return feature_vector\n",
    "\n",
    "feature_vector = create_feats(prints=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_feats(prints=True):\n",
    "#     group_thresholds = {\n",
    "#         # -------------------------------- feature --------------------- | -- Threshold --\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i].lower(), t]):                         0,     # mandatory feature f100\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i][-4:].lower(), t]):                    5,     # mandatory feature f101\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i][-3:].lower(), t]):                    5,     # mandatory feature f101\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i][-2:].lower(), t]):                    5,     # mandatory feature f101\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i][-1:].lower(), t]):                    5,     # mandatory feature f101\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i][:4].lower(), t]):                     5,     # mandatory feature f102\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i][:3].lower(), t]):                     5,     # mandatory feature f102\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i][:2].lower(), t]):                     5,     # mandatory feature f102\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i][:1].lower(), t]):                     5,     # mandatory feature f102\n",
    "#         lambda t2, t1, w, i, t: tuple([t2, t1, t]):                               1,     # mandatory feature f103\n",
    "#         lambda t2, t1, w, i, t: tuple([t1, t]):                                   1,     # mandatory feature f104\n",
    "#         lambda t2, t1, w, i, t: tuple([t]):                                       1,     # mandatory feature f105\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i].islower(), t]):                       1,     # mandatory feature has_uppercase (inverted)\n",
    "#         lambda t2, t1, w, i, t: tuple([any(char.isdigit() for char in w[i]), t]): 1,     # mandatory feature has_digits\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i-1].lower(), t]):                       20,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i+1].lower(), t]):                       20,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i+1][:3].lower(), t]):                   20,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i-1][:3].lower(), t]):                   20,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i+1][:2].lower(), t]):                   20,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i-1][:2].lower(), t]):                   20,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i+1][-3:].lower(), t]):                  20,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i-1][-3:].lower(), t]):                  20,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i+1][-2:].lower(), t]):                  20,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i-1][-2:].lower(), t]):                  20,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i].isalnum(), t]):                       1,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i].isalpha(), t]):                       1,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i].isascii(), t]):                       1,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i].isdecimal(), t]):                     1,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i].isdigit(), t]):                       1,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i].isnumeric(), t]):                     1,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i].istitle(), t]):                       1,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i].isupper(), t]):                       1,\n",
    "#         lambda t2, t1, w, i, t: tuple([len(w[i]), t]):                            1,\n",
    "#     }\n",
    "\n",
    "#     tic = time.time()\n",
    "#     feature_vector = features.create_feature_vector(dataset=train_dataset,\n",
    "#                                                     group_thresholds=group_thresholds,\n",
    "#                                                     pruning=True,\n",
    "#                                                     get_stats=False,\n",
    "#                                                     assertions=True,\n",
    "#                                                     calls_counter=False)\n",
    "\n",
    "#     if prints:\n",
    "#         print('feature_vector creation time:', time.time() - tic, 'sec\\n')\n",
    "#         for feat in feature_vector.feats:\n",
    "#             print('feat_group:', feat, '| feats:', len(feat))\n",
    "#     print('feat_groups:', len(feature_vector.feats), '| total_feats:', len(feature_vector))\n",
    "#     return feature_vector\n",
    "\n",
    "# feature_vector = create_feats(prints=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_feats(prints=True):\n",
    "#     group_thresholds = {\n",
    "#         # -------------------------------- feature --------------------- | -- Threshold --\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i].lower(), t]):                         0,     # mandatory feature f100\n",
    "# #         lambda t2, t1, w, i, t: tuple([w[i-1].lower(), w[i].lower(), w[i+1].lower(), t]):                 10,\n",
    "# #         lambda t2, t1, w, i, t: tuple([w[i-2].lower(), w[i-1].lower(), w[i].lower(), t]):                 10,\n",
    "# #         lambda t2, t1, w, i, t: tuple([w[i].lower(), w[i+1].lower(), w[i+2].lower(), t]):                 10,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i].lower(), t]):                         0,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i][-4:].lower(), t]):                    5,     # mandatory feature f101\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i][-3:].lower(), t]):                    5,     # mandatory feature f101\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i][-2:].lower(), t]):                    5,     # mandatory feature f101\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i][-1:].lower(), t]):                    5,     # mandatory feature f101\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i][:4].lower(), t]):                     5,     # mandatory feature f102\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i][:3].lower(), t]):                     5,     # mandatory feature f102\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i][:2].lower(), t]):                     5,     # mandatory feature f102\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i][:1].lower(), t]):                     5,     # mandatory feature f102\n",
    "#         lambda t2, t1, w, i, t: tuple([t2, t1, t]):                               1,     # mandatory feature f103\n",
    "#         lambda t2, t1, w, i, t: tuple([t1, t]):                                   1,     # mandatory feature f104\n",
    "#         lambda t2, t1, w, i, t: tuple([t]):                                       0,     # mandatory feature f105\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i].islower(), t]):                       0,     # mandatory feature has_uppercase (inverted)\n",
    "#         lambda t2, t1, w, i, t: tuple([any(char.isdigit() for char in w[i]), t]): 0,     # mandatory feature has_digits\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i-1].lower(), t]):                       20,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i+1].lower(), t]):                       20,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i+1][:3].lower(), t]):                   20,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i-1][:3].lower(), t]):                   20,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i+1][:2].lower(), t]):                   20,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i-1][:2].lower(), t]):                   20,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i+1][-3:].lower(), t]):                  20,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i-1][-3:].lower(), t]):                  20,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i+1][-2:].lower(), t]):                  20,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i-1][-2:].lower(), t]):                  20,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i].isalnum(), t]):                       0,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i].isalpha(), t]):                       0,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i].isascii(), t]):                       0,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i].isdecimal(), t]):                     0,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i].isdigit(), t]):                       0,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i].isnumeric(), t]):                     0,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i].istitle(), t]):                       0,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i].isupper(), t]):                       0,\n",
    "#         lambda t2, t1, w, i, t: tuple(['-' in w[i], t]):                          0,\n",
    "#         lambda t2, t1, w, i, t: tuple(['.' in w[i], t]):                          0,\n",
    "#         lambda t2, t1, w, i, t: tuple([',' in w[i], t]):                          0,\n",
    "#         lambda t2, t1, w, i, t: tuple(['\\/' in w[i], t]):                         0,\n",
    "#         lambda t2, t1, w, i, t: tuple([len(w[i]), t]):                            0,\n",
    "#         lambda t2, t1, w, i, t: tuple([len(w), t]):                               5,\n",
    "#         lambda t2, t1, w, i, t: tuple([i, t]):                                    10,\n",
    "#         lambda t2, t1, w, i, t: tuple([i==len(w)-1, t]):                          0,\n",
    "\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i-1].isalnum(), t]):                     0,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i-1].isalpha(), t]):                     0,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i-1].isascii(), t]):                     0,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i-1].isdecimal(), t]):                   0,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i-1].isdigit(), t]):                     0,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i-1].isnumeric(), t]):                   0,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i-1].istitle(), t]):                     0,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i-1].isupper(), t]):                     0,\n",
    "\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i+1].isalnum(), t]):                     0,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i+1].isalpha(), t]):                     0,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i+1].isascii(), t]):                     0,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i+1].isdecimal(), t]):                   0,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i+1].isdigit(), t]):                     0,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i+1].isnumeric(), t]):                   0,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i+1].istitle(), t]):                     0,\n",
    "#         lambda t2, t1, w, i, t: tuple([w[i+1].isupper(), t]):                     0,\n",
    "#     }\n",
    "\n",
    "#     tic = time.time()\n",
    "#     feature_vector = features.create_feature_vector(dataset=train_dataset,\n",
    "#                                                     group_thresholds=group_thresholds,\n",
    "#                                                     pruning=True,\n",
    "#                                                     get_stats=False,\n",
    "#                                                     assertions=True,\n",
    "#                                                     calls_counter=False)\n",
    "\n",
    "#     if prints:\n",
    "#         print('feature_vector creation time:', time.time() - tic, 'sec\\n')\n",
    "#         for feat in feature_vector.feats:\n",
    "#             print('feat_group:', feat, '| feats:', len(feat))\n",
    "#     print('feat_groups:', len(feature_vector.feats), '| total_feats:', len(feature_vector))\n",
    "#     return feature_vector\n",
    "\n",
    "# feature_vector = create_feats(prints=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def w0_uniform_0_1_centered_normalized():\n",
    "#     w0 = np.random.rand(len(feature_vector))\n",
    "#     w0 -= w0.mean()\n",
    "#     w0 /= w0.std()\n",
    "#     return w0\n",
    "\n",
    "# def w0_uniform_0_1_normalized():\n",
    "#     w0 = np.random.rand(len(feature_vector))\n",
    "#     w0 /= w0.std()\n",
    "#     return w0\n",
    "\n",
    "# def w0_uniform_0_1_centered():\n",
    "#     w0 = np.random.rand(len(feature_vector))\n",
    "#     w0 -= w0.mean()\n",
    "#     return w0\n",
    "\n",
    "# def w0_uniform_0_1():\n",
    "#     return np.random.rand(len(feature_vector))\n",
    "\n",
    "# def w0_xavier():\n",
    "#     return np.random.randn(len(feature_vector))*np.sqrt(1/len(feature_vector))\n",
    "\n",
    "# def w0_zero():\n",
    "#     return np.zeros(len(feature_vector)).astype(np.float32)\n",
    "\n",
    "# w0_init = w0_uniform_0_1\n",
    "# w0 = w0_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = classifier.load_model(version=12,\n",
    "#                               models_path=models_path,\n",
    "#                               epoch=112,\n",
    "#                               seed=42,\n",
    "#                               prints=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version = 2.1\n",
    "\n",
    "# model = classifier.Model(version=version,\n",
    "#                          w0=w0_init(),\n",
    "#                          tags=train_dataset.tags,\n",
    "#                          inference=classifier.viterbi,\n",
    "#                          feature_vector=feature_vector,\n",
    "#                          score_func=metrics.accuracy,\n",
    "#                          models_path=models_path,\n",
    "#                          max_weights_history=5,\n",
    "#                          save=False)\n",
    "\n",
    "# # model.load(weights=True, weights_history=True, feature_vector=True, log=True, epoch=epoch, prints=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  44/143 | train_loss 1.063742 | val_loss 8.247883 | train_score 0.000000 | val_score 0.9264 | train_time  50.15 min\n",
      "epoch  45/143 | train_loss 1.569618 | val_loss 9.283092 | train_score 0.000000 | val_score 0.9117 | train_time  64.09 min\n",
      "epoch  46/143 | train_loss 0.971807 | val_loss 8.146290 | train_score 0.000000 | val_score 0.9250 | train_time  78.14 min\n",
      "epoch  47/143 | train_loss 0.956656 | val_loss 8.116944 | train_score 0.000000 | val_score 0.9250 | train_time  92.07 min\n",
      "epoch  48/143 | train_loss 0.921604 | val_loss 8.064214 | train_score 0.000000 | val_score 0.9252 | train_time 106.09 min\n",
      "epoch  49/143 | train_loss 0.899271 | val_loss 8.046641 | train_score 0.000000 | val_score 0.9254 | train_time 120.07 min\n",
      "epoch  50/143 | train_loss 0.850193 | val_loss 8.040568 | train_score 0.000000 | val_score 0.9252 | train_time 134.08 min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/mnt/c/Users/alexz/OneDrive/MainEnv/nlp/MEMM_Part_of_Speech_Tagging/utils/features.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, t2, t1, w, i, t)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfoo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhash_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalls_counter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ('by', 'VBG')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-4eb5811b52ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m                                   \u001b[0mtrain_aprox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_aprox\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                                   \u001b[0mval_aprox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_aprox\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                                   batch_growth=batch_growth)\n\u001b[0m",
      "\u001b[0;32m/mnt/c/Users/alexz/OneDrive/MainEnv/nlp/MEMM_Part_of_Speech_Tagging/utils/classifier.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, train_dataset, val_dataset, batch_size, weight_decay, save, tqdm_bar, beam, train_aprox, val_aprox, batch_growth)\u001b[0m\n\u001b[1;32m    228\u001b[0m                                                      \u001b[0mmaxfun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                                                      \u001b[0mmaxiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                                                      iprint=-1)\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv_min\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_min\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/nlp_hw1/lib/python3.7/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[0;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[0;32m--> 199\u001b[0;31m                            **opts)\n\u001b[0m\u001b[1;32m    200\u001b[0m     d = {'grad': res['jac'],\n\u001b[1;32m    201\u001b[0m          \u001b[0;34m'task'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/nlp_hw1/lib/python3.7/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/nlp_hw1/lib/python3.7/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/nlp_hw1/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/nlp_hw1/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/alexz/OneDrive/MainEnv/nlp/MEMM_Part_of_Speech_Tagging/utils/classifier.py\u001b[0m in \u001b[0;36m_loss_and_grad\u001b[0;34m(v, model, epochs, train_dataset, val_dataset, train, weight_decay, batch_size, save, tqdm_bar, beam, train_aprox, val_aprox, batch_growth)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         val_loss = _loss_and_grad(v=v, model=model, epochs=0, train_dataset=None, val_dataset=val_dataset, train=False, weight_decay=weight_decay, batch_size=None,\n\u001b[0;32m--> 317\u001b[0;31m                                   save=False, tqdm_bar=tqdm_bar, beam=0, train_aprox=0, val_aprox=0, batch_growth=None) if val_dataset is not None else 0.0\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mtrain_aprox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrain_aprox\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtrain_aprox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/alexz/OneDrive/MainEnv/nlp/MEMM_Part_of_Speech_Tagging/utils/classifier.py\u001b[0m in \u001b[0;36m_loss_and_grad\u001b[0;34m(v, model, epochs, train_dataset, val_dataset, train, weight_decay, batch_size, save, tqdm_bar, beam, train_aprox, val_aprox, batch_growth)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mfeat_vec_tag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_list_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'both'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m                 \u001b[0mfeat_list_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'list'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;31m# grad expected_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/alexz/OneDrive/MainEnv/nlp/MEMM_Part_of_Speech_Tagging/utils/features.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, t2, t1, w, i, t, fmt)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mfeat_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                     \u001b[0mfeat_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/alexz/OneDrive/MainEnv/nlp/MEMM_Part_of_Speech_Tagging/utils/features.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, t2, t1, w, i, t)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalls\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhash_calls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_save = True  # save model after each training epoch, if False model will need to be saved manually\n",
    "beam = 1  # viterbi beam size for model evaluation during training\n",
    "train_aprox = 0  # aproximate train_score with first train_aprox train samples\n",
    "val_aprox = 1000  # aproximate val_score with first val_aprox train samples \n",
    "weight_decay = 0.0  # lamda regularization parameter\n",
    "init_batch_size = 250  # batch_size for loader\n",
    "batch_growth = 0\n",
    "epochs = 100  # training epochs\n",
    "tqdm_bar = False  # display tqdm progress bars\n",
    "\n",
    "v_min, f_min, d_min = model.train(epochs=epochs,\n",
    "                                  train_dataset=train_dataset,\n",
    "                                  val_dataset=val_dataset,\n",
    "                                  batch_size=init_batch_size,\n",
    "                                  weight_decay=weight_decay,\n",
    "                                  save=train_save,\n",
    "                                  tqdm_bar=tqdm_bar,\n",
    "                                  beam=beam,\n",
    "                                  train_aprox=train_aprox,\n",
    "                                  val_aprox=val_aprox,\n",
    "                                  batch_growth=batch_growth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # recalculate best INDs\n",
    "# best_loss = 9999.9\n",
    "# for i in model.log.index:\n",
    "#     best = model.log.loc[i]['val_loss'] < best_loss\n",
    "#     if best:\n",
    "#         model.log.loc[i, 'best'] = True\n",
    "#         best_loss = model.log.loc[i]['val_loss']\n",
    "# model.save(epoch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 2.1\n",
    "weights_history = []\n",
    "for epoch in [43,42,41,40]:\n",
    "    model = classifier.load_model(version=version,\n",
    "                                  models_path=models_path,\n",
    "                                  epoch=epoch,\n",
    "                                  seed=42,\n",
    "                                  prints=False)\n",
    "    weights_history.append(model.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.feature_vector = model43.feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model version: 2.1\n",
      "epochs: 43\n",
      "train_time: 36.064\n",
      "\n",
      "last train_loss: 1.063742\n",
      "last val_loss: 8.247883\n",
      "last train_score: 0.000000\n",
      "last val_score: 0.926393\n",
      "best val_score: 0.9264 at epoch 28\n",
      "4548\n",
      "4548\n"
     ]
    }
   ],
   "source": [
    "version = 2.1\n",
    "epoch = 43\n",
    "model = classifier.load_model(version=version,\n",
    "                              models_path=models_path,\n",
    "                              epoch=epoch,\n",
    "                              seed=42,\n",
    "                              prints=True)\n",
    "print(len(model.feature_vector))\n",
    "print(len(model.weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.74132932, 1.04591833, 1.28413936, ..., 1.73102991, 2.84690286,\n",
       "       1.28347268])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74132932 1.04591833 1.28413936 ... 1.73102991 2.84690286 1.28347268]\n",
      "[0.72027096 1.03981328 1.26418465 ... 1.68628278 2.81311107 1.25848098]\n",
      "[0.7150308  1.03812774 1.25851461 ... 1.67604707 2.79869899 1.25145026]\n",
      "[0.70444969 1.03510677 1.24736011 ... 1.65080721 2.7653069  1.23783151]\n"
     ]
    }
   ],
   "source": [
    "for weights in weights_history:\n",
    "    print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights_history = weights_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.avg_weights(save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [12:33<00:00,  1.33it/s, acc=0.9249]\n"
     ]
    }
   ],
   "source": [
    "aprox_num = 1000  # max samples to aproximate score\n",
    "display_all = False\n",
    "beam_stats = {}\n",
    "dataset = val_dataset\n",
    "\n",
    "for predict_beam in [1]:  # viterbi beam size\n",
    "    beam_stats[predict_beam] = {}\n",
    "    beam_stats[predict_beam]['pred_tags'] = []\n",
    "    beam_stats[predict_beam]['true_tags'] = []\n",
    "    pbar = tqdm(dataset.sentences[:aprox_num])\n",
    "    for sentence in pbar:\n",
    "        preds = model(sentence[0], predict_beam)\n",
    "        beam_stats[predict_beam]['pred_tags'].append(preds)\n",
    "        beam_stats[predict_beam]['true_tags'].append(sentence[1])\n",
    "\n",
    "        if display_all:\n",
    "            display(pd.DataFrame((sentence[0], sentence[1], preds), index=('words', 'tags', 'preds')))\n",
    "        pbar.set_postfix(acc='{:.4f}'.format(model.score_func(beam_stats[predict_beam]['pred_tags'],\n",
    "                                              beam_stats[predict_beam]['true_tags'])),\n",
    "                         refresh=False)\n",
    "\n",
    "    beam_stats[predict_beam]['matrix'], beam_stats[predict_beam]['worst'] = \\\n",
    "                                        metrics.confusion_matrix(tags,\n",
    "                                                                 beam_stats[predict_beam]['pred_tags'],\n",
    "                                                                 beam_stats[predict_beam]['true_tags'])\n",
    "    model.comp_preds = beam_stats\n",
    "    comp_acc = model.score_func(model.comp_preds[predict_beam]['pred_tags'],\n",
    "                                model.comp_preds[predict_beam]['true_tags'])\n",
    "    model.save(manual_path='{}/V{:}/model_V{:}_E0{:}_beam{:}_comp_acc{:.4f}.pth'.format(models_path,\n",
    "                                                                                        model.version,\n",
    "                                                                                        model.version,\n",
    "                                                                                        model.get_log(),\n",
    "                                                                                        predict_beam,\n",
    "                                                                                        comp_acc))\n",
    "\n",
    "# description = f\"comp_acc={comp_acc}, {len(feature_vector.feats)} feat_groups, {len(feature_vector)} total_feats\"\n",
    "# logger.log(model, w0_init, create_feats, load_datasets, description, manual_cols = {'val_score': comp_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model version: 1.2\n",
      "epochs: 92\n",
      "train_time: 1013.684\n",
      "\n",
      "last train_loss: 1.613323\n",
      "last val_loss: 3.170205\n",
      "last train_score: 0.000000\n",
      "last val_score: 0.927181\n",
      "best val_score: 0.9272 at epoch 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [23:46<00:00,  1.43s/it, acc=0.9532]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'PRP$'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.miniconda3/envs/nlp_hw1/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'PRP$'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-27ccac7e30cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m                                         metrics.confusion_matrix(tags,\n\u001b[1;32m     30\u001b[0m                                                                  \u001b[0mbeam_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredict_beam\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pred_tags'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                                                                  beam_stats[predict_beam]['true_tags'])\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomp_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeam_stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     comp_acc = model.score_func(model.comp_preds[predict_beam]['pred_tags'],\n",
      "\u001b[0;32m/mnt/c/Users/alexz/OneDrive/MainEnv/nlp/MEMM_Part_of_Speech_Tagging/utils/metrics.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(tags, pred_tags, true_tags)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrues\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/nlp_hw1/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2994\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2995\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2996\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2997\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/nlp_hw1/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'PRP$'"
     ]
    }
   ],
   "source": [
    "model = classifier.load_model(version=1.2,\n",
    "                              models_path=models_path,\n",
    "                              epoch=92,\n",
    "                              seed=42,\n",
    "                              prints=True)\n",
    "\n",
    "aprox_num = 1000  # max samples to aproximate score\n",
    "display_all = False\n",
    "beam_stats = {}\n",
    "dataset = preprocess.Dataset('data/test1.wtag')\n",
    "\n",
    "for predict_beam in [1]:  # viterbi beam size\n",
    "    beam_stats[predict_beam] = {}\n",
    "    beam_stats[predict_beam]['pred_tags'] = []\n",
    "    beam_stats[predict_beam]['true_tags'] = []\n",
    "    pbar = tqdm(dataset.sentences[:aprox_num])\n",
    "    for sentence in pbar:\n",
    "        preds = model(sentence[0], predict_beam)\n",
    "        beam_stats[predict_beam]['pred_tags'].append(preds)\n",
    "        beam_stats[predict_beam]['true_tags'].append(sentence[1])\n",
    "\n",
    "        if display_all:\n",
    "            display(pd.DataFrame((sentence[0], sentence[1], preds), index=('words', 'tags', 'preds')))\n",
    "        pbar.set_postfix(acc='{:.4f}'.format(model.score_func(beam_stats[predict_beam]['pred_tags'],\n",
    "                                              beam_stats[predict_beam]['true_tags'])),\n",
    "                         refresh=False)\n",
    "\n",
    "    beam_stats[predict_beam]['matrix'], beam_stats[predict_beam]['worst'] = \\\n",
    "                                        metrics.confusion_matrix(tags,\n",
    "                                                                 beam_stats[predict_beam]['pred_tags'],\n",
    "                                                                 beam_stats[predict_beam]['true_tags'])\n",
    "    model.comp_preds = beam_stats\n",
    "    comp_acc = model.score_func(model.comp_preds[predict_beam]['pred_tags'],\n",
    "                                model.comp_preds[predict_beam]['true_tags'])\n",
    "    model.save(manual_path='{}/V{:}/model_V{:}_E0{:}_beam{:}_test1_acc{:.4f}.pth'.format(models_path,\n",
    "                                                                                         model.version,\n",
    "                                                                                         model.version,\n",
    "                                                                                         model.get_log(),\n",
    "                                                                                         predict_beam,\n",
    "                                                                                         comp_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_beam = 1\n",
    "beam_stats[predict_beam]['matrix'], beam_stats[predict_beam]['worst'] = \\\n",
    "                                    metrics.confusion_matrix(tags,\n",
    "                                                             beam_stats[predict_beam]['pred_tags'],\n",
    "                                                             beam_stats[predict_beam]['true_tags'])\n",
    "model.comp_preds = beam_stats\n",
    "comp_acc = model.score_func(model.comp_preds[predict_beam]['pred_tags'],\n",
    "                            model.comp_preds[predict_beam]['true_tags'])\n",
    "model.save(manual_path='{}/V{:}/model_V{:}_E0{:}_beam{:}_test1_acc{:.4f}.pth'.format(models_path,\n",
    "                                                                                     model.version,\n",
    "                                                                                     model.version,\n",
    "                                                                                     model.get_log(),\n",
    "                                                                                     predict_beam,\n",
    "                                                                                     comp_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = []\n",
    "for i, (preds, trues) in enumerate(zip(best_model.comp_preds[1]['pred_tags'], best_model.comp_preds[1]['true_tags'])):\n",
    "#     sentence_scores.append((i, best_model.score_func(preds, trues)))\n",
    "    sentence_scores.append(best_model.score_func(preds, trues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9270752521334368"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = 50\n",
    "best_model.score_func(best_model.comp_preds[1]['pred_tags'][:num], best_model.comp_preds[1]['true_tags'][:num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model version: 2.1\n",
      "epochs: 44\n",
      "train_time: 40.407\n",
      "\n",
      "last train_loss: 1.376101\n",
      "last val_loss: 6.598743\n",
      "last train_score: 0.000000\n",
      "last val_score: 0.000000\n",
      "best val_score: 0.0000 at epoch 44\n"
     ]
    }
   ],
   "source": [
    "# model = classifier.load_model(version=2.1,\n",
    "#                               models_path=models_path,\n",
    "#                               epoch=44,\n",
    "#                               seed=42,\n",
    "#                               prints=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [12:37<2:56:47, 757.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 val_score 0.9214880976115475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [25:09<2:43:48, 756.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 31 val_score 0.918657043741776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [38:08<2:32:32, 762.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 32 val_score 0.9202918776665736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4/15 [50:42<2:19:22, 760.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 33 val_score 0.9210893576298895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5/15 [1:03:20<2:06:35, 759.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34 val_score 0.9219267115913713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [1:16:17<1:54:43, 764.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 35 val_score 0.922764065552853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [1:29:21<1:42:44, 770.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 36 val_score 0.9220064595877029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [1:41:50<1:29:08, 764.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 37 val_score 0.9228039395510188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9/15 [1:54:21<1:15:59, 759.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 38 val_score 0.9234419235216715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10/15 [2:07:19<1:03:47, 765.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 39 val_score 0.9241596554886559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11/15 [2:20:26<51:27, 771.87s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40 val_score 0.9224051995693608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 12/15 [2:33:34<38:50, 776.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 41 val_score 0.9236811675106663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13/15 [2:46:22<25:48, 774.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 42 val_score 0.924518521472148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [2:59:07<12:51, 771.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 43 val_score 0.9263925993859404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [3:20:28<00:00, 801.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 44 val_score 0.9252761274372981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# aprox_num = 1000  # max samples to aproximate score\n",
    "# beam = 1\n",
    "# # weights_history = []\n",
    "# version = 2.1\n",
    "# start_epoch = 30\n",
    "# end_epoch = 44\n",
    "# val_scores = {}\n",
    "# # val_losses = {}\n",
    "\n",
    "# for epoch in tqdm(range(start_epoch, end_epoch + 1)):\n",
    "#     model = classifier.Model(version=version,\n",
    "#                              w0=w0.copy(),\n",
    "#                              tags=train_dataset.tags,\n",
    "#                              inference=classifier.viterbi,\n",
    "#                              feature_vector=feature_vector,\n",
    "#                              score_func=metrics.accuracy,\n",
    "#                              models_path=models_path,\n",
    "#                              max_weights_history=5,\n",
    "#                              save=False)\n",
    "#     model.load(weights=True, weights_history=False, feature_vector=True, log=True, epoch=epoch, prints=False)\n",
    "# #     weights_history.append(model.weights.copy())\n",
    "# #     weights_history = weights_history[-5:]\n",
    "# #     model.weights_history = weights_history\n",
    "    \n",
    "#     val_score = model.score_func(*model.predict(val_dataset.sentences[:aprox_num], beam, tqdm_bar=False))\n",
    "# #     val_loss = classifier._loss_and_grad(v=model.weights,\n",
    "# #                                          model=model,\n",
    "# #                                          epochs=0,\n",
    "# #                                          train_dataset=None,\n",
    "# #                                          val_dataset=val_dataset,\n",
    "# #                                          train=False,\n",
    "# #                                          weight_decay=0.0,\n",
    "# #                                          batch_size=None,\n",
    "# #                                          save=False,\n",
    "# #                                          tqdm_bar=False,\n",
    "# #                                          beam=0,\n",
    "# #                                          train_aprox=0,\n",
    "# #                                          val_aprox=0,\n",
    "# #                                          batch_growth=None)\n",
    "#     val_scores[epoch] = val_score\n",
    "# #     val_losses[epoch] = val_loss\n",
    "#     print('epoch', epoch, 'val_score', val_score)\n",
    "    \n",
    "#     for epoch in val_scores:\n",
    "#         model.log.loc[epoch, 'val_score'] = val_scores[epoch]\n",
    "#         model.log.loc[epoch, 'val_aprox'] = aprox_num\n",
    "# #         model.log.loc[epoch, 'val_loss'] = val_losses[epoch]\n",
    "# #     model.version = new_version\n",
    "#     model.save(epoch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model version: 2.1\n",
      "epochs: 44\n",
      "train_time: 40.407\n",
      "\n",
      "last train_loss: 1.376101\n",
      "last val_loss: 6.598743\n",
      "last train_score: 0.000000\n",
      "last val_score: 0.000000\n",
      "best val_score: 0.0000 at epoch 44\n"
     ]
    }
   ],
   "source": [
    "# best_model = classifier.load_model(from_file='model_V12_E087_comp_acc0.9247.pth')\n",
    "best_model = classifier.load_model(from_file='model_V2.1_E044_comp_acc0.9253.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7838\n",
      "7838\n"
     ]
    }
   ],
   "source": [
    "print(len(best_model.feature_vector))\n",
    "print(len(best_model.weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp1_tagged = []\n",
    "for i in range(len(val_dataset.sentences)):\n",
    "    joined_sentence = []\n",
    "    assert len(val_dataset.sentences[i][0]) == len(best_model.comp_preds[1]['pred_tags'][i])\n",
    "    for word, pred in zip(val_dataset.sentences[i][0], best_model.comp_preds[1]['pred_tags'][i]):\n",
    "        joined_sentence.append('_'.join([word, pred]))\n",
    "    comp1_tagged.append(' '.join(joined_sentence))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/comp_m1_321128258.wtag', 'w') as f:\n",
    "    for row in comp1_tagged:\n",
    "        f.write(row)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.avg_weights(save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23674/23674 [00:34<00:00, 694.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.449555919838531"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1_dataset = preprocess.Dataset('data/test1.wtag')\n",
    "val_loss = classifier._loss_and_grad(v=model.weights,\n",
    "                                     model=model,\n",
    "                                     epochs=0,\n",
    "                                     train_dataset=None,\n",
    "                                     val_dataset=test1_dataset,\n",
    "                                     train=False,\n",
    "                                     weight_decay=0.0,\n",
    "                                     batch_size=None,\n",
    "                                     save=False,\n",
    "                                     tqdm_bar=True,\n",
    "                                     beam=0,\n",
    "                                     train_aprox=0,\n",
    "                                     val_aprox=0,\n",
    "                                     batch_growth=None)\n",
    "val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.plot(['val_loss', 'train_loss'], 'loss', 'loss', scale='linear', basey=10)\n",
    "model.plot(['batch_size'], 'batch_size', 'batch_size', scale='log', basey=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = classifier.load_model(from_file='checkpoint_V11_E081_0.952_SEED42.pth')\n",
    "best_model.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NN</th>\n",
       "      <th>JJ</th>\n",
       "      <th>NNP</th>\n",
       "      <th>RB</th>\n",
       "      <th>IN</th>\n",
       "      <th>VBN</th>\n",
       "      <th>NNS</th>\n",
       "      <th>VBD</th>\n",
       "      <th>VB</th>\n",
       "      <th>VBP</th>\n",
       "      <th>...</th>\n",
       "      <th>WP</th>\n",
       "      <th>-LRB-</th>\n",
       "      <th>TO</th>\n",
       "      <th>:</th>\n",
       "      <th>-RRB-</th>\n",
       "      <th>WRB</th>\n",
       "      <th>EX</th>\n",
       "      <th>WP$</th>\n",
       "      <th>``</th>\n",
       "      <th>SYM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NN</th>\n",
       "      <td>3133</td>\n",
       "      <td>94</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JJ</th>\n",
       "      <td>112</td>\n",
       "      <td>1299</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNP</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>1916</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RB</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>713</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IN</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>2479</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBN</th>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>431</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNS</th>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1432</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBD</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>791</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VB</th>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>540</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBP</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>283</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNPS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBG</th>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JJR</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBZ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RBR</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DT</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RP</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CD</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WDT</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JJS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RBS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PDT</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MD</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UH</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FW</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRP</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>''</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRP$</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WP</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-LRB-</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TO</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>524</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>:</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-RRB-</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WRB</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EX</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WP$</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>``</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SYM</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         NN    JJ   NNP   RB    IN  VBN   NNS  VBD   VB  VBP  ...  WP  -LRB-  \\\n",
       "NN     3133    94    15   11     3    5     9    3   18   12  ...   0      0   \n",
       "JJ      112  1299    23   22     1   28     1    1    7    6  ...   0      0   \n",
       "NNP      26    26  1916    0     3    0     2    0    1    0  ...   0      0   \n",
       "RB        3    12     1  713    52    0     0    0    2    1  ...   0      0   \n",
       "IN        2     1     0   18  2479    0     0    0    2    2  ...   0      0   \n",
       "VBN       0    32     0    0     0  431     0   31    0    0  ...   0      0   \n",
       "NNS      35     2     8    1     0    0  1432    0    0    0  ...   0      0   \n",
       "VBD       2     8     1    0     0   33     0  791    2    6  ...   0      0   \n",
       "VB       16     8     0    6     1    1     0    1  540   16  ...   0      0   \n",
       "VBP       7     1     0    0     0    0     0    7   11  283  ...   0      0   \n",
       "NNPS      0     0    18    0     0    0     1    0    0    0  ...   0      0   \n",
       "VBG      26    12     0    0     1    0     0    0    1    1  ...   0      0   \n",
       "JJR       2     0     0    1     1    0     0    0    1    0  ...   0      0   \n",
       "VBZ       0     0     0    0     0    0    14    0    0    1  ...   0      0   \n",
       "RBR       0     0     0    2     0    0     0    0    0    0  ...   0      0   \n",
       "DT        0     0     2    4     3    0     0    0    0    0  ...   0      0   \n",
       "RP        1     0     0    3    12    0     0    0    0    0  ...   0      0   \n",
       "CD        6     1     2    0     0    0     1    0    0    0  ...   0      0   \n",
       "WDT       0     0     0    0     4    0     0    0    0    0  ...   0      0   \n",
       "JJS       0     0     0    1     0    0     0    0    0    0  ...   0      0   \n",
       "RBS       0     0     0    0     0    0     0    0    0    0  ...   0      0   \n",
       "POS       0     0     0    0     0    0     0    0    0    0  ...   0      0   \n",
       "PDT       0     1     0    0     0    0     0    0    0    0  ...   0      0   \n",
       "#         0     0     0    0     0    0     0    0    0    0  ...   0      0   \n",
       "MD        1     0     0    0     0    0     0    0    0    0  ...   0      0   \n",
       "UH        0     0     0    0     0    0     0    0    0    0  ...   0      0   \n",
       "FW        0     0     0    0     0    0     0    0    0    0  ...   0      0   \n",
       "CC        0     0     0    2     0    0     0    0    0    0  ...   0      0   \n",
       "PRP       0     0     0    0     0    0     0    0    0    0  ...   0      0   \n",
       "''        0     0     0    0     0    0     0    0    0    0  ...   0      0   \n",
       "PRP$      0     0     0    0     0    0     1    0    0    0  ...   0      0   \n",
       "$         0     0     0    0     0    0     0    0    0    0  ...   0      0   \n",
       ".         0     0     0    0     0    0     0    0    0    0  ...   0      0   \n",
       ",         0     0     0    0     0    0     0    0    0    0  ...   0      0   \n",
       "WP        0     0     0    0     0    0     0    0    0    0  ...  52      0   \n",
       "-LRB-     0     0     0    0     0    0     0    0    0    0  ...   0     35   \n",
       "TO        0     0     0    0     0    0     0    0    0    0  ...   0      0   \n",
       ":         0     0     0    0     0    0     0    0    0    0  ...   0      0   \n",
       "-RRB-     0     0     0    0     0    0     0    0    0    0  ...   0      0   \n",
       "WRB       0     0     0    0     0    0     0    0    0    0  ...   0      0   \n",
       "EX        0     0     0    0     0    0     0    0    0    0  ...   0      0   \n",
       "WP$       0     0     0    0     0    0     0    0    0    0  ...   0      0   \n",
       "``        0     0     0    0     0    0     0    0    0    0  ...   0      0   \n",
       "SYM       0     0     0    0     0    0     0    0    0    0  ...   0      0   \n",
       "\n",
       "        TO    :  -RRB-  WRB  EX  WP$   ``  SYM  \n",
       "NN       0    0      0    0   0    0    0    0  \n",
       "JJ       0    0      0    0   0    0    0    0  \n",
       "NNP      0    0      0    0   0    0    0    0  \n",
       "RB       0    0      0    0   0    0    0    0  \n",
       "IN       0    0      0    0   0    0    0    0  \n",
       "VBN      0    0      0    0   0    0    0    0  \n",
       "NNS      0    0      0    0   0    0    0    0  \n",
       "VBD      0    0      0    0   0    0    0    0  \n",
       "VB       0    0      0    0   0    0    0    0  \n",
       "VBP      0    0      0    0   0    0    0    0  \n",
       "NNPS     0    0      0    0   0    0    0    0  \n",
       "VBG      0    0      0    0   0    0    0    0  \n",
       "JJR      0    0      0    0   0    0    0    0  \n",
       "VBZ      0    0      0    0   0    0    0    0  \n",
       "RBR      0    0      0    0   0    0    0    0  \n",
       "DT       0    0      0    0   0    0    0    0  \n",
       "RP       0    0      0    0   0    0    0    0  \n",
       "CD       0    0      0    0   0    0    0    0  \n",
       "WDT      0    0      0    0   0    0    0    0  \n",
       "JJS      0    0      0    0   0    0    0    0  \n",
       "RBS      0    0      0    0   0    0    0    0  \n",
       "POS      0    0      0    0   0    0    0    0  \n",
       "PDT      0    0      0    0   0    0    0    0  \n",
       "#        0    0      0    0   0    0    0    0  \n",
       "MD       0    0      0    0   0    0    0    0  \n",
       "UH       0    0      0    0   0    0    0    0  \n",
       "FW       0    0      0    0   0    0    0    0  \n",
       "CC       0    0      0    0   0    0    0    0  \n",
       "PRP      0    0      0    0   0    0    0    0  \n",
       "''       0    0      0    0   0    0    0    0  \n",
       "PRP$     0    0      0    0   0    0    0    0  \n",
       "$        0    0      0    0   0    0    0    0  \n",
       ".        0    0      0    0   0    0    0    0  \n",
       ",        0    0      0    0   0    0    0    0  \n",
       "WP       0    0      0    0   0    0    0    0  \n",
       "-LRB-    0    0      0    0   0    0    0    0  \n",
       "TO     524    0      0    0   0    0    0    0  \n",
       ":        0  115      0    0   0    0    0    0  \n",
       "-RRB-    0    0     35    0   0    0    0    0  \n",
       "WRB      0    0      0   56   0    0    0    0  \n",
       "EX       0    0      0    0  24    0    0    0  \n",
       "WP$      0    0      0    0   0    8    0    0  \n",
       "``       0    0      0    0   0    0  150    0  \n",
       "SYM      0    0      0    0   0    0    0    0  \n",
       "\n",
       "[44 rows x 44 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.beam_stats[1]['matrix'][worst_tags].loc[worst_tags].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NN': 0.9806116414632086,\n",
       " 'JJ': 0.9823434991974318,\n",
       " 'NNP': 0.9930725690631073,\n",
       " 'RB': 0.9935372138210695,\n",
       " 'IN': 0.9941285798766579,\n",
       " 'VBN': 0.9943397820393681,\n",
       " 'NNS': 0.9948044267973304,\n",
       " 'VBD': 0.9959871589085072,\n",
       " 'VB': 0.9960293993410493,\n",
       " 'VBP': 0.9968319675593478,\n",
       " 'NNPS': 0.9969164484244318,\n",
       " 'VBG': 0.9969586888569739,\n",
       " 'JJR': 0.9977190166427304,\n",
       " 'VBZ': 0.9980146996705246,\n",
       " 'RBR': 0.998479344428487,\n",
       " 'DT': 0.9986483061586551,\n",
       " 'RP': 0.9987750274562811,\n",
       " 'CD': 0.9991129509166173,\n",
       " 'WDT': 0.9992396722142435,\n",
       " 'JJS': 0.9993663935118695,\n",
       " 'RBS': 0.9995353552420376,\n",
       " 'POS': 0.9996620765396638,\n",
       " 'PDT': 0.9997465574047478,\n",
       " '#': 0.9997887978372898,\n",
       " 'MD': 0.9997887978372898,\n",
       " 'UH': 0.9998310382698319,\n",
       " 'FW': 0.9998732787023739,\n",
       " 'CC': 0.9998732787023739,\n",
       " 'PRP': 0.9999155191349159,\n",
       " \"''\": 0.9999155191349159,\n",
       " 'PRP$': 0.9999155191349159,\n",
       " '$': 0.9999577595674579,\n",
       " '.': 0.9999577595674579,\n",
       " ',': 1.0,\n",
       " 'WP': 1.0,\n",
       " '-LRB-': 1.0,\n",
       " 'TO': 1.0,\n",
       " ':': 1.0,\n",
       " '-RRB-': 1.0,\n",
       " 'WRB': 1.0,\n",
       " 'EX': 1.0,\n",
       " 'WP$': 1.0,\n",
       " '``': 1.0,\n",
       " 'SYM': 1.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.beam_stats[1]['worst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.beam_stats = beam_stats\n",
    "model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(matrix1)\n",
    "display(matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(worst1)\n",
    "display(worst2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code that may be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = {}\n",
    "# for sentence in train_dataset.sentences:\n",
    "#     sentences[len(sentence[0])] = sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# sample_sentence = (['Terms', 'were', \"n't\", 'disclosed', '.'],  # sentence words\n",
    "#                    ['NNS',   'VBD',  'RB',  'VBN',       '.'])  # sentence true tags\n",
    "# predict_beam = 100  # viterbi beam size\n",
    "\n",
    "# tags, bp_pi = viterbi(model, sample_sentence[0], beam=predict_beam)\n",
    "# print('sentence ', sample_sentence[0])\n",
    "# print('true tags', sample_sentence[1])\n",
    "# print('pred tags', tags)\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3802186.0\n",
      "760.4372\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32871</th>\n",
       "      <td>16939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42333</th>\n",
       "      <td>16939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42378</th>\n",
       "      <td>16939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42289</th>\n",
       "      <td>16939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42423</th>\n",
       "      <td>16939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7471</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7472</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13388</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7476</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8908</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42783 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        feat\n",
       "32871  16939\n",
       "42333  16939\n",
       "42378  16939\n",
       "42289  16939\n",
       "42423  16939\n",
       "...      ...\n",
       "7471       1\n",
       "7472       1\n",
       "13388      1\n",
       "7476       1\n",
       "8908       1\n",
       "\n",
       "[42783 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.2 s, sys: 15.6 ms, total: 28.3 s\n",
      "Wall time: 28.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sum_vec = np.zeros(len(feature_vector)).astype(np.float32)\n",
    "sum_inds = 0\n",
    "for t2, t1, w, i, t in train_dataset:\n",
    "    vec = feature_vector(t2, t1, w, i, t, fmt='vec')\n",
    "    sum_vec += vec\n",
    "    sum_inds += (vec).sum()\n",
    "\n",
    "df = pd.DataFrame(sum_vec, columns=['feat']).astype({'feat': int}).sort_values('feat', ascending=False)\n",
    "print(sum_vec.sum())\n",
    "print(sum_inds/len(train_dataset.sentences))\n",
    "display(df.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17008</th>\n",
       "      <td>6199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19968</th>\n",
       "      <td>6199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42480</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24990</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19758</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20012</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26421</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42299</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42179</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32881</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22556</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42388</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33000</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32927</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17056</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42433</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42667</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18812</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42239</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14771</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42611</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42735</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42343</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42554</th>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42655</th>\n",
       "      <td>5104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42606</th>\n",
       "      <td>4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42383</th>\n",
       "      <td>4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42174</th>\n",
       "      <td>4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32921</th>\n",
       "      <td>4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42294</th>\n",
       "      <td>4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42549</th>\n",
       "      <td>4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32995</th>\n",
       "      <td>4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42474</th>\n",
       "      <td>4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42662</th>\n",
       "      <td>4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32876</th>\n",
       "      <td>4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42234</th>\n",
       "      <td>4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42428</th>\n",
       "      <td>4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42728</th>\n",
       "      <td>4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42338</th>\n",
       "      <td>4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31974</th>\n",
       "      <td>4934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18787</th>\n",
       "      <td>4914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14742</th>\n",
       "      <td>4914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19740</th>\n",
       "      <td>4914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24962</th>\n",
       "      <td>4914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19988</th>\n",
       "      <td>4914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17030</th>\n",
       "      <td>4914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22530</th>\n",
       "      <td>4914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       feat\n",
       "0      6199\n",
       "17008  6199\n",
       "19968  6199\n",
       "42480  6044\n",
       "24990  6044\n",
       "19758  6044\n",
       "20012  6044\n",
       "56     6044\n",
       "26421  6044\n",
       "42299  6044\n",
       "42179  6044\n",
       "32881  6044\n",
       "22556  6044\n",
       "42388  6044\n",
       "33000  6044\n",
       "32927  6044\n",
       "17056  6044\n",
       "42433  6044\n",
       "42667  6044\n",
       "18812  6044\n",
       "42239  6044\n",
       "14771  6044\n",
       "42611  6044\n",
       "42735  6044\n",
       "42343  6044\n",
       "42554  6044\n",
       "42655  5104\n",
       "42606  4962\n",
       "42383  4962\n",
       "42174  4962\n",
       "32921  4962\n",
       "42294  4962\n",
       "42549  4962\n",
       "32995  4962\n",
       "42474  4962\n",
       "42662  4962\n",
       "32876  4962\n",
       "42234  4962\n",
       "42428  4962\n",
       "42728  4962\n",
       "42338  4962\n",
       "31974  4934\n",
       "18787  4914\n",
       "14742  4914\n",
       "19740  4914\n",
       "24962  4914\n",
       "23     4914\n",
       "19988  4914\n",
       "17030  4914\n",
       "22530  4914"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.loc[0:100].head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureGroup(tuple([w[i].lower(), t]))\n",
      "('low-density', 'NN')\n"
     ]
    }
   ],
   "source": [
    "feat, key = feature_vector.invert_feat(4914)  # 41453 22811\n",
    "print(feat)\n",
    "print(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test run train_dataset\n",
    "# tic = time.time()\n",
    "# for t2, t1, w, i, t in train_dataset:\n",
    "#     feat_vec_t = feature_vector(t2, t1, w, i, t, fmt='vec')\n",
    "# print('fmt=vec: {:.3f} sec'.format(time.time() - tic))\n",
    "\n",
    "# tic = time.time()\n",
    "# for t2, t1, w, i, t in train_dataset:\n",
    "#     feat_list_t = feature_vector(t2, t1, w, i, t, fmt='list')\n",
    "# print('fmt=list: {:.3f} sec'.format(time.time() - tic))\n",
    "\n",
    "# tic = time.time()\n",
    "# for t2, t1, w, i, t in train_dataset:\n",
    "#     feat_vec_t, feat_list_t = feature_vector(t2, t1, w, i, t, fmt='both')\n",
    "# print('fmt=vec+list: {:.3f} sec'.format(time.time() - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tag in train1_statistics.words_per_tag:\n",
    "#     if len(train1_statistics.words_per_tag[tag]) < 10:\n",
    "#         print('{:5} tf: {:5d} unique_count: {:4d} words: {}'.format(tag, train1_statistics.tags_count[tag], len(train1_statistics.words_per_tag[tag]),\n",
    "#                                                                     train1_statistics.words_per_tag[tag]))\n",
    "#     else:\n",
    "#         print('{:5} tf: {:5d} unique_count: {:4d}'.format(tag, train1_statistics.tags_count[tag], len(train1_statistics.words_per_tag[tag])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = []\n",
    "\n",
    "# # one-to-one features\n",
    "# for word in strange_words:\n",
    "#     features.append(Feature(f'w[i] == \"{word}\"', t=train1_model.tags_per_word[word][0]))\n",
    "#     print(word, train1_model.WordCount[word], train1_model.TagsPerWord[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_hw1",
   "language": "python",
   "name": "nlp_hw1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
