{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import optimize\n",
    "from utils import preprocess, score, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params, maybe need to remove that\n",
    "model_name = 'model.pickle'\n",
    "model_matrices = 'model_matrices.pickle'\n",
    "model_preprocess = 'model_preprocess.pickle'\n",
    "load_model = False\n",
    "load_matrices = False\n",
    "load_preprocess = False\n",
    "verbose = 1\n",
    "\n",
    "# variables\n",
    "limit_common_words = 5\n",
    "threshold = 0\n",
    "args = None\n",
    "lamda = 0.1\n",
    "\n",
    "# data files\n",
    "comp1_path = 'data/comp1.words'\n",
    "comp2_path = 'data/comp2.words'\n",
    "test1_path = 'data/test1.wtag'\n",
    "train1_path = 'data/train1.wtag'\n",
    "train2_path = 'data/train2.wtag'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.92 s, sys: 219 ms, total: 10.1 s\n",
      "Wall time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train1_statistics = preprocess.feature_statistics_class(train1_path, limit_common_words, lamda)\n",
    "train1_statistics.get_statistics()\n",
    "# train2_statistics = feature_statistics_class(train2_path, limit_common_words, lamda)\n",
    "# train2_statistics.get_statistics()\n",
    "id_class = preprocess.feature2id_class(train1_statistics, threshold)\n",
    "id_class.get_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{',', 'JJ', '-LRB-', 'VBG', 'VBP', 'NNPS', 'WP', 'VB', 'TO', 'CC', 'SYM', '-RRB-', 'PRP$', 'PRP', '#', 'MD', 'WRB', 'VBN', ':', '.', 'JJR', 'EX', 'UH', 'RBR', 'NN', 'POS', 'VBZ', \"''\", 'WDT', 'NNP', 'JJS', '``', 'IN', '*', 'RB', 'NNS', 'RBS', 'FW', 'VBD', 'DT', 'RP', '$', 'CD', 'PDT', 'WP$'}\n"
     ]
    }
   ],
   "source": [
    "all_tags = set(train1_statistics.tags)\n",
    "print(all_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create FeatureGroup params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_name = 'wi_t'\n",
    "hash_rules = ('w[i]', 't')\n",
    "comb_template = ('word', 'tag')\n",
    "\n",
    "combs_dict = dict()\n",
    "for word in train1_statistics.TagsPerWord:\n",
    "    for tag in train1_statistics.TagsPerWord[word]:\n",
    "        comb = tuple([eval(feat) for feat in comb_template])\n",
    "        if comb in combs_dict:\n",
    "            combs_dict[comb] += 1\n",
    "        else:\n",
    "            combs_dict[comb] = 1\n",
    "\n",
    "all_features[group_name] = features.FeatureGroup(hash_rules, set(combs_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_name = 'pre3_wi_t'\n",
    "hash_rules = ('prefix(w[i], 3)', 't')\n",
    "comb_template = ('features.prefix(word, 3)', 'tag')\n",
    "\n",
    "combs_dict = dict()\n",
    "for word in train1_statistics.TagsPerWord:\n",
    "    for tag in train1_statistics.TagsPerWord[word]:\n",
    "        comb = tuple([eval(feat) for feat in comb_template])\n",
    "        if comb in combs_dict:\n",
    "            combs_dict[comb] += 1\n",
    "        else:\n",
    "            combs_dict[comb] = 1\n",
    "\n",
    "all_features[group_name] = features.FeatureGroup(hash_rules, set(combs_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_name = 'pre2_wi_t'\n",
    "hash_rules = ('prefix(w[i], 2)', 't')\n",
    "comb_template = ('features.prefix(word, 2)', 'tag')\n",
    "\n",
    "combs_dict = dict()\n",
    "for word in train1_statistics.TagsPerWord:\n",
    "    for tag in train1_statistics.TagsPerWord[word]:\n",
    "        comb = tuple([eval(feat) for feat in comb_template])\n",
    "        if comb in combs_dict:\n",
    "            combs_dict[comb] += 1\n",
    "        else:\n",
    "            combs_dict[comb] = 1\n",
    "\n",
    "all_features[group_name] = features.FeatureGroup(hash_rules, set(combs_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_name = 'suf3_wi_t'\n",
    "hash_rules = ('suffix(w[i], 3)', 't')\n",
    "comb_template = ('features.suffix(word, 3)', 'tag')\n",
    "\n",
    "combs_dict = dict()\n",
    "for word in train1_statistics.TagsPerWord:\n",
    "    for tag in train1_statistics.TagsPerWord[word]:\n",
    "        comb = tuple([eval(feat) for feat in comb_template])\n",
    "        if comb in combs_dict:\n",
    "            combs_dict[comb] += 1\n",
    "        else:\n",
    "            combs_dict[comb] = 1\n",
    "\n",
    "all_features[group_name] = features.FeatureGroup(hash_rules, set(combs_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_name = 'suf2_wi_t'\n",
    "hash_rules = ('suffix(w[i], 2)', 't')\n",
    "comb_template = ('features.suffix(word, 2)', 'tag')\n",
    "\n",
    "combs_dict = dict()\n",
    "for word in train1_statistics.TagsPerWord:\n",
    "    for tag in train1_statistics.TagsPerWord[word]:\n",
    "        comb = tuple([eval(feat) for feat in comb_template])\n",
    "        if comb in combs_dict:\n",
    "            combs_dict[comb] += 1\n",
    "        else:\n",
    "            combs_dict[comb] = 1\n",
    "\n",
    "all_features[group_name] = features.FeatureGroup(hash_rules, set(combs_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_name = 't1_t'\n",
    "hash_rules = ('t1', 't')\n",
    "comb_template = 'tuple(sentence[i: i + 2])'\n",
    "\n",
    "combs_dict = dict()\n",
    "for sentence in train1_statistics.sentences_with_only_tag:\n",
    "    sentence = ['*', '*'] + sentence\n",
    "    for i in range(len(sentence) - 1):\n",
    "        comb = eval(comb_template)\n",
    "        if comb in combs_dict:\n",
    "            combs_dict[comb] += 1\n",
    "        else:\n",
    "            combs_dict[comb] = 1\n",
    "\n",
    "all_features[group_name] = features.FeatureGroup(hash_rules, set(combs_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_name = 't2_t1_t'\n",
    "hash_rules = ('t2', 't1', 't')\n",
    "comb_template = 'tuple(sentence[i: i + 3])'\n",
    "\n",
    "combs_dict = dict()\n",
    "for sentence in train1_statistics.sentences_with_only_tag:\n",
    "    sentence = ['*', '*'] + sentence\n",
    "    for i in range(len(sentence) - 2):\n",
    "        comb = eval(comb_template)\n",
    "        if comb in combs_dict:\n",
    "            combs_dict[comb] += 1\n",
    "        else:\n",
    "            combs_dict[comb] = 1\n",
    "\n",
    "all_features[group_name] = features.FeatureGroup(hash_rules, set(combs_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_group: wi_t | feats: 15415\n",
      "feat_group: pre3_wi_t | feats: 8061\n",
      "feat_group: pre2_wi_t | feats: 3009\n",
      "feat_group: suf3_wi_t | feats: 11844\n",
      "feat_group: suf2_wi_t | feats: 13612\n",
      "feat_group: t1_t | feats: 1061\n",
      "feat_group: t2_t1_t | feats: 8150\n",
      "feat_groups: 7 | total_feats: 61152\n"
     ]
    }
   ],
   "source": [
    "total_feats = 0\n",
    "for feat in all_features:\n",
    "    feat_len = len(all_features[feat].hash_dict)\n",
    "    total_feats += feat_len\n",
    "    print('feat_group:', feat, '| feats:', feat_len)\n",
    "print('feat_groups:', len(all_features), '| total_feats:', total_feats)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "wi_t 15415\n",
    "pre3_wi_t 8061\n",
    "pre2_wi_t 3009\n",
    "suf3_wi_t 11844\n",
    "suf2_wi_t 13612\n",
    "t1_t 1061\n",
    "t2_t1_t 8150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". 4914 ['.']\n",
      "`` 838 ['``']\n",
      ", 6044 [',']\n",
      "'' 813 [\"''\"]\n",
      "$ 773 ['$']\n",
      "-- 256 [':']\n",
      "-LRB- 148 ['-LRB-']\n",
      "-RRB- 148 ['-RRB-']\n",
      ": 149 [':']\n",
      "-LCB- 30 ['-LRB-']\n",
      "-RCB- 30 ['-RRB-']\n",
      "; 133 [':']\n",
      "... 19 [':']\n",
      "C$ 13 ['$']\n",
      "US$ 15 ['$']\n",
      "A$ 1 ['$']\n",
      "? 43 ['.']\n",
      "! 5 ['.']\n",
      "` 8 ['``']\n",
      "- 8 [':']\n",
      "# 7 ['#']\n",
      "HK$ 6 ['$']\n"
     ]
    }
   ],
   "source": [
    "# features = []\n",
    "\n",
    "# # one-to-one features\n",
    "# for word in strange_words:\n",
    "#     features.append(Feature(f'w[i] == \"{word}\"', t=train1_model.TagsPerWord[word][0]))\n",
    "#     print(word, train1_model.WordCount[word], train1_model.TagsPerWord[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = 'VB'\n",
    "t1 = 'NN'\n",
    "w = ('preprocess', 'abc', 'being')\n",
    "i = 1\n",
    "t = 'BB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(feat(t2, t1, w, i, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_hw1",
   "language": "python",
   "name": "nlp_hw1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
