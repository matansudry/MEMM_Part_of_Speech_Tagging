{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import preprocess, features, classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp1_path = 'data/comp1.words'\n",
    "comp2_path = 'data/comp2.words'\n",
    "test1_path = 'data/test1.wtag'\n",
    "train1_path = 'data/train1.wtag'\n",
    "train2_path = 'data/train2.wtag'\n",
    "\n",
    "train_dataset = preprocess.Dataset(train1_path)\n",
    "val_dataset = preprocess.Dataset(test1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features, only used when load = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_thresholds = {\n",
    "    lambda t2, t1, w, i, t: tuple([t2, t1, t]):                               1,\n",
    "    lambda t2, t1, w, i, t: tuple([t1, t]):                                   1,\n",
    "    lambda t2, t1, w, i, t: tuple([w[i].lower(), t]):                         None,\n",
    "#     lambda t2, t1, w, i, t: tuple([w[i-1].lower(), t]):                       5,\n",
    "#     lambda t2, t1, w, i, t: tuple([w[i+1].lower(), t]):                       5,\n",
    "    lambda t2, t1, w, i, t: tuple([w[i][:3].lower(), t]):                     5,\n",
    "    lambda t2, t1, w, i, t: tuple([w[i][:2].lower(), t]):                     5,\n",
    "#     lambda t2, t1, w, i, t: tuple([w[i+1][:3].lower(), t]):                   5,\n",
    "#     lambda t2, t1, w, i, t: tuple([w[i-1][:3].lower(), t]):                   5,\n",
    "    lambda t2, t1, w, i, t: tuple([w[i][-3:].lower(), t]):                    5,\n",
    "    lambda t2, t1, w, i, t: tuple([w[i][-2:].lower(), t]):                    5,\n",
    "#     lambda t2, t1, w, i, t: tuple([w[i+1][-3:].lower(), t]):                  5,\n",
    "#     lambda t2, t1, w, i, t: tuple([w[i-1][-3:].lower(), t]):                  5,\n",
    "    lambda t2, t1, w, i, t: tuple([w[i].isalnum(), t]):                       1,\n",
    "    lambda t2, t1, w, i, t: tuple([w[i].isalpha(), t]):                       1,\n",
    "    lambda t2, t1, w, i, t: tuple([w[i].isascii(), t]):                       1,\n",
    "    lambda t2, t1, w, i, t: tuple([w[i].isdecimal(), t]):                     1,\n",
    "    lambda t2, t1, w, i, t: tuple([w[i].isdigit(), t]):                       1,\n",
    "    lambda t2, t1, w, i, t: tuple([w[i].islower(), t]):                       1,\n",
    "    lambda t2, t1, w, i, t: tuple([w[i].isnumeric(), t]):                     1,\n",
    "    lambda t2, t1, w, i, t: tuple([w[i].istitle(), t]):                       1,\n",
    "    lambda t2, t1, w, i, t: tuple([w[i].isupper(), t]):                       1,\n",
    "    lambda t2, t1, w, i, t: tuple([len(w[i]) == 1, t]):                       1,\n",
    "    lambda t2, t1, w, i, t: tuple([len(w[i]) == 2, t]):                       1,\n",
    "    lambda t2, t1, w, i, t: tuple([len(w[i]) == 3, t]):                       1,\n",
    "    lambda t2, t1, w, i, t: tuple([w[i][0].islower(), t]):                    1,\n",
    "    lambda t2, t1, w, i, t: tuple([any(char.isdigit() for char in w[i]), t]): 1,\n",
    "    lambda t2, t1, w, i, t: tuple([any(char.isupper() for char in w[i]), t]): 1,\n",
    "}\n",
    "\n",
    "feature_vector = features.create_feature_vector(train_dataset,\n",
    "                                                group_thresholds=group_thresholds,\n",
    "                                                pruning=True,\n",
    "                                                get_stats=False,\n",
    "                                                assertions=True,\n",
    "                                                calls_counter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model version: beam-50_train_aprox-0_val_aprox-0_weight_decay-1_batch_size-5000_max_epochs-50\n",
      "epochs: 23\n",
      "train_time: 160.301\n",
      "\n",
      "last train_loss: 54.828313\n",
      "last val_loss: 56.020626\n",
      "last train_score: nan\n",
      "last val_score: nan\n",
      "best val_score: nan at epoch 0\n"
     ]
    }
   ],
   "source": [
    "w0 = np.random.rand(len(feature_vector)).astype(np.float32)  # init weights\n",
    "seed = 42  # seed for batch shuffle for loader\n",
    "models_path = 'models'  # folder of models\n",
    "train_save = True  # save model after each training epoch\n",
    "load = True  # load last weights, log and feature_vector into model\n",
    "train = False  # perform a training session\n",
    "beam = 50  # viterbi beam size for model evaluation during training\n",
    "train_aprox = 0  # max train samples to aproximate train_score\n",
    "val_aprox = 0  # max val samples to aproximate val_score\n",
    "weight_decay = 1  # lamda regularization parameter\n",
    "batch_size = 5000  # batch_size for loader\n",
    "epochs = 50  # training epochs\n",
    "tqdm_bar = False  # display tqdm progress bars\n",
    "# generated model version ID\n",
    "version = f\"beam-{beam}_train_aprox-{train_aprox}_val_aprox-{val_aprox}_\" + \\\n",
    "          f\"weight_decay-{weight_decay}_batch_size-{batch_size}_max_epochs-{epochs}\"\n",
    "\n",
    "model = classifier.Model(version=version,\n",
    "                         w0=w0,\n",
    "                         tags=train_dataset.tags,\n",
    "                         inference=classifier.viterbi,\n",
    "                         feature_vector=feature_vector,\n",
    "                         seed=seed,\n",
    "                         score_func=classifier.accuracy,\n",
    "                         models_path=models_path,\n",
    "                         save=False)\n",
    "\n",
    "if load:\n",
    "    model.load(weights=True, feature_vector=True, log=True, epoch=-1, prints=True)\n",
    "\n",
    "if train:\n",
    "    v_min, f_min, d_min = model.train(epochs=epochs,\n",
    "                                      train_dataset=train_dataset,\n",
    "                                      val_dataset=val_dataset,\n",
    "                                      batch_size=batch_size,\n",
    "                                      weight_decay=weight_decay,\n",
    "                                      iprint=-1,\n",
    "                                      save=train_save,\n",
    "                                      tqdm_bar=tqdm_bar,\n",
    "                                      beam=beam,\n",
    "                                      train_aprox=train_aprox,\n",
    "                                      val_aprox=val_aprox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manual model evaluation on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [25:13<00:00, 30.26s/it]\n"
     ]
    }
   ],
   "source": [
    "aprox_num = 100  # max samples to aproximate score\n",
    "predict_beam = 50  # viterbi beam size\n",
    "dataset = val_dataset  # dataset to evaluate\n",
    "tqdm_bar = True  # display tqdm progress bar\n",
    "\n",
    "pred_tags, true_tags = model.predict(dataset.sentences[:aprox_num],\n",
    "                                           predict_beam=predict_beam,\n",
    "                                           tqdm_bar=tqdm_bar)\n",
    "score = model.score_func(pred_tags, true_tags)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manual model evaluation on a sample_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence  ['Terms', 'were', \"n't\", 'disclosed', '.']\n",
      "true tags ['NNS', 'VBD', 'RB', 'VBN', '.']\n",
      "pred tags ['NNP', 'NNP', 'IN', 'DT', 'NN']\n",
      "CPU times: user 10.2 s, sys: 0 ns, total: 10.2 s\n",
      "Wall time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sample_sentence = (['Terms', 'were', \"n't\", 'disclosed', '.'],  # sentence words\n",
    "                   ['NNS',   'VBD',  'RB',  'VBN',       '.'])  # sentence true tags\n",
    "predict_beam = 100  # viterbi beam size\n",
    "\n",
    "tags, bp_pi = viterbi(model, sample_sentence[0], beam=predict_beam)\n",
    "print('sentence ', sample_sentence[0])\n",
    "print('true tags', sample_sentence[1])\n",
    "print('pred tags', tags)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code that may be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = {}\n",
    "# for sentence in train_dataset.sentences:\n",
    "#     sentences[len(sentence[0])] = sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for feat in feature_vector.feats:\n",
    "#     print('feat_group:', feat, '| feats:', len(feat))\n",
    "# print('feat_groups:', len(feature_vector.feats), '| total_feats:', len(feature_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test run train_dataset\n",
    "# tic = time.time()\n",
    "# for t2, t1, w, i, t in train_dataset:\n",
    "#     feat_vec_t = feature_vector(t2, t1, w, i, t, fmt='vec')\n",
    "# print('fmt=vec: {:.3f} sec'.format(time.time() - tic))\n",
    "\n",
    "# tic = time.time()\n",
    "# for t2, t1, w, i, t in train_dataset:\n",
    "#     feat_list_t = feature_vector(t2, t1, w, i, t, fmt='list')\n",
    "# print('fmt=list: {:.3f} sec'.format(time.time() - tic))\n",
    "\n",
    "# tic = time.time()\n",
    "# for t2, t1, w, i, t in train_dataset:\n",
    "#     feat_vec_t, feat_list_t = feature_vector(t2, t1, w, i, t, fmt='both')\n",
    "# print('fmt=vec+list: {:.3f} sec'.format(time.time() - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tag in train1_statistics.words_per_tag:\n",
    "#     if len(train1_statistics.words_per_tag[tag]) < 10:\n",
    "#         print('{:5} tf: {:5d} unique_count: {:4d} words: {}'.format(tag, train1_statistics.tags_count[tag], len(train1_statistics.words_per_tag[tag]),\n",
    "#                                                                     train1_statistics.words_per_tag[tag]))\n",
    "#     else:\n",
    "#         print('{:5} tf: {:5d} unique_count: {:4d}'.format(tag, train1_statistics.tags_count[tag], len(train1_statistics.words_per_tag[tag])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = []\n",
    "\n",
    "# # one-to-one features\n",
    "# for word in strange_words:\n",
    "#     features.append(Feature(f'w[i] == \"{word}\"', t=train1_model.tags_per_word[word][0]))\n",
    "#     print(word, train1_model.WordCount[word], train1_model.TagsPerWord[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_hw1",
   "language": "python",
   "name": "nlp_hw1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
